{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_9_Introduction_to_Loss_Functions_HAS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hassanmushtaq524/FutureMakers22/blob/main/Day_9_Introduction_to_Loss_Functions_HAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec29cba0-a1ee-412b-f0cb-672d1a36578a"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "print(train_features.shape)\n",
        "print(train_labels)\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
            " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
            " 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7\n",
            " 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.\n",
            " 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3\n",
            " 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.\n",
            " 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5\n",
            " 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.\n",
            " 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2\n",
            " 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.\n",
            " 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9\n",
            " 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7\n",
            " 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7\n",
            " 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6\n",
            " 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8\n",
            " 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9\n",
            " 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7\n",
            " 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3\n",
            " 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.\n",
            " 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9\n",
            " 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6\n",
            " 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8\n",
            " 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.\n",
            " 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.\n",
            " 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.\n",
            " 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2\n",
            " 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4\n",
            " 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4\n",
            " 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64736103-2723-4095-def2-d472dd42c298"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 4s 17ms/step - loss: 588.6125 - mse: 588.6125 - val_loss: 491.1870 - val_mse: 491.1870\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 570.5316 - mse: 570.5316 - val_loss: 473.7460 - val_mse: 473.7460\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 544.8559 - mse: 544.8559 - val_loss: 447.2063 - val_mse: 447.2063\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 505.8602 - mse: 505.8602 - val_loss: 406.2992 - val_mse: 406.2992\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 450.9902 - mse: 450.9902 - val_loss: 347.6400 - val_mse: 347.6400\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 373.7946 - mse: 373.7946 - val_loss: 270.6099 - val_mse: 270.6099\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 280.3833 - mse: 280.3833 - val_loss: 181.2339 - val_mse: 181.2339\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 183.3351 - mse: 183.3351 - val_loss: 106.4573 - val_mse: 106.4573\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 114.2408 - mse: 114.2408 - val_loss: 72.3358 - val_mse: 72.3358\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 88.1099 - mse: 88.1099 - val_loss: 60.5186 - val_mse: 60.5186\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 75.9748 - mse: 75.9748 - val_loss: 50.2903 - val_mse: 50.2903\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 66.1402 - mse: 66.1402 - val_loss: 42.0505 - val_mse: 42.0505\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 58.8299 - mse: 58.8299 - val_loss: 36.0004 - val_mse: 36.0004\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 53.4968 - mse: 53.4968 - val_loss: 31.5740 - val_mse: 31.5740\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 49.1167 - mse: 49.1167 - val_loss: 28.7417 - val_mse: 28.7417\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 45.7071 - mse: 45.7071 - val_loss: 26.2964 - val_mse: 26.2964\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 42.6059 - mse: 42.6059 - val_loss: 23.8518 - val_mse: 23.8518\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 39.9835 - mse: 39.9835 - val_loss: 22.1193 - val_mse: 22.1193\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 37.4500 - mse: 37.4500 - val_loss: 20.8817 - val_mse: 20.8817\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 35.4227 - mse: 35.4227 - val_loss: 19.4997 - val_mse: 19.4997\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 33.4192 - mse: 33.4192 - val_loss: 18.4559 - val_mse: 18.4559\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.5653 - mse: 31.5653 - val_loss: 17.3420 - val_mse: 17.3420\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29.9227 - mse: 29.9227 - val_loss: 16.7269 - val_mse: 16.7269\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.4508 - mse: 28.4508 - val_loss: 16.0722 - val_mse: 16.0722\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 26.9978 - mse: 26.9978 - val_loss: 15.7109 - val_mse: 15.7109\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 25.7483 - mse: 25.7483 - val_loss: 15.1708 - val_mse: 15.1708\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 24.7740 - mse: 24.7740 - val_loss: 14.5403 - val_mse: 14.5403\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23.7939 - mse: 23.7939 - val_loss: 14.2126 - val_mse: 14.2126\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.7342 - mse: 22.7342 - val_loss: 13.8151 - val_mse: 13.8151\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.9389 - mse: 21.9389 - val_loss: 13.4478 - val_mse: 13.4478\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.1327 - mse: 21.1327 - val_loss: 13.3175 - val_mse: 13.3175\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.4741 - mse: 20.4741 - val_loss: 13.2561 - val_mse: 13.2561\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.9135 - mse: 19.9135 - val_loss: 12.9240 - val_mse: 12.9240\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.5836 - mse: 19.5836 - val_loss: 12.8719 - val_mse: 12.8719\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.6646 - mse: 18.6646 - val_loss: 12.7922 - val_mse: 12.7922\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.3607 - mse: 18.3607 - val_loss: 12.3871 - val_mse: 12.3871\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.8712 - mse: 17.8712 - val_loss: 12.3013 - val_mse: 12.3013\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.3810 - mse: 17.3810 - val_loss: 12.2435 - val_mse: 12.2435\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.0395 - mse: 17.0395 - val_loss: 12.2448 - val_mse: 12.2448\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.6591 - mse: 16.6591 - val_loss: 11.9914 - val_mse: 11.9914\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.3311 - mse: 16.3311 - val_loss: 11.8751 - val_mse: 11.8751\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.1906 - mse: 16.1906 - val_loss: 11.6749 - val_mse: 11.6749\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.7208 - mse: 15.7208 - val_loss: 11.6041 - val_mse: 11.6041\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.4521 - mse: 15.4521 - val_loss: 11.4775 - val_mse: 11.4775\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.1661 - mse: 15.1661 - val_loss: 11.4226 - val_mse: 11.4226\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.8421 - mse: 14.8421 - val_loss: 11.2299 - val_mse: 11.2299\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.6615 - mse: 14.6615 - val_loss: 11.3047 - val_mse: 11.3047\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.4450 - mse: 14.4450 - val_loss: 11.3020 - val_mse: 11.3020\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.3026 - mse: 14.3026 - val_loss: 11.2798 - val_mse: 11.2798\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.0425 - mse: 14.0425 - val_loss: 11.0916 - val_mse: 11.0916\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.9764 - mse: 13.9764 - val_loss: 10.8588 - val_mse: 10.8588\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7423 - mse: 13.7423 - val_loss: 10.9467 - val_mse: 10.9467\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5654 - mse: 13.5654 - val_loss: 10.9675 - val_mse: 10.9675\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.4624 - mse: 13.4624 - val_loss: 10.7578 - val_mse: 10.7578\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.3454 - mse: 13.3454 - val_loss: 10.7602 - val_mse: 10.7602\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.1844 - mse: 13.1844 - val_loss: 10.5449 - val_mse: 10.5449\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0968 - mse: 13.0968 - val_loss: 10.4863 - val_mse: 10.4863\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.9473 - mse: 12.9473 - val_loss: 10.3802 - val_mse: 10.3802\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.9562 - mse: 12.9562 - val_loss: 10.3727 - val_mse: 10.3727\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.7491 - mse: 12.7491 - val_loss: 10.4190 - val_mse: 10.4190\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.7091 - mse: 12.7091 - val_loss: 10.2019 - val_mse: 10.2019\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6366 - mse: 12.6366 - val_loss: 10.0724 - val_mse: 10.0724\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.5002 - mse: 12.5002 - val_loss: 9.9331 - val_mse: 9.9331\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.4643 - mse: 12.4643 - val_loss: 9.8828 - val_mse: 9.8828\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3614 - mse: 12.3614 - val_loss: 9.9074 - val_mse: 9.9074\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.3134 - mse: 12.3134 - val_loss: 9.7988 - val_mse: 9.7988\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.1825 - mse: 12.1825 - val_loss: 9.9187 - val_mse: 9.9187\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.0950 - mse: 12.0950 - val_loss: 9.8123 - val_mse: 9.8123\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.0295 - mse: 12.0295 - val_loss: 9.7519 - val_mse: 9.7519\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.9613 - mse: 11.9613 - val_loss: 9.7344 - val_mse: 9.7344\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.9274 - mse: 11.9274 - val_loss: 9.8113 - val_mse: 9.8113\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.8283 - mse: 11.8283 - val_loss: 9.7896 - val_mse: 9.7896\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7249 - mse: 11.7249 - val_loss: 9.8655 - val_mse: 9.8655\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7062 - mse: 11.7062 - val_loss: 9.9806 - val_mse: 9.9806\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.6952 - mse: 11.6952 - val_loss: 9.9763 - val_mse: 9.9763\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.6981 - mse: 11.6981 - val_loss: 9.8249 - val_mse: 9.8249\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.6050 - mse: 11.6050 - val_loss: 9.6846 - val_mse: 9.6846\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11.5040 - mse: 11.5040 - val_loss: 9.6085 - val_mse: 9.6085\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5179 - mse: 11.5179 - val_loss: 9.7119 - val_mse: 9.7119\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.4191 - mse: 11.4191 - val_loss: 9.6525 - val_mse: 9.6525\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3706 - mse: 11.3706 - val_loss: 9.6141 - val_mse: 9.6141\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.2866 - mse: 11.2866 - val_loss: 9.6975 - val_mse: 9.6975\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2083 - mse: 11.2083 - val_loss: 9.5302 - val_mse: 9.5302\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2330 - mse: 11.2330 - val_loss: 9.4712 - val_mse: 9.4712\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1913 - mse: 11.1913 - val_loss: 9.3973 - val_mse: 9.3973\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.0487 - mse: 11.0487 - val_loss: 9.1947 - val_mse: 9.1947\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.0234 - mse: 11.0234 - val_loss: 9.1633 - val_mse: 9.1633\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1335 - mse: 11.1335 - val_loss: 9.2571 - val_mse: 9.2571\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9297 - mse: 10.9297 - val_loss: 9.2358 - val_mse: 9.2358\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9392 - mse: 10.9392 - val_loss: 9.2426 - val_mse: 9.2426\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.0695 - mse: 11.0695 - val_loss: 9.4478 - val_mse: 9.4478\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9246 - mse: 10.9246 - val_loss: 9.4205 - val_mse: 9.4205\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7047 - mse: 10.7047 - val_loss: 9.2804 - val_mse: 9.2804\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7506 - mse: 10.7506 - val_loss: 9.2061 - val_mse: 9.2061\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9220 - mse: 10.9220 - val_loss: 9.0395 - val_mse: 9.0395\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7875 - mse: 10.7875 - val_loss: 9.1058 - val_mse: 9.1058\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5730 - mse: 10.5730 - val_loss: 9.0810 - val_mse: 9.0810\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5058 - mse: 10.5058 - val_loss: 9.1252 - val_mse: 9.1252\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5287 - mse: 10.5287 - val_loss: 8.9648 - val_mse: 8.9648\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5349 - mse: 10.5349 - val_loss: 8.9160 - val_mse: 8.9160\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.3976 - mse: 10.3976 - val_loss: 8.9394 - val_mse: 8.9394\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3574 - mse: 10.3574 - val_loss: 8.9629 - val_mse: 8.9629\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2855 - mse: 10.2855 - val_loss: 8.8169 - val_mse: 8.8169\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3554 - mse: 10.3554 - val_loss: 8.7377 - val_mse: 8.7377\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.2211 - mse: 10.2211 - val_loss: 8.9160 - val_mse: 8.9160\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.2174 - mse: 10.2174 - val_loss: 8.8577 - val_mse: 8.8577\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.2324 - mse: 10.2324 - val_loss: 8.8673 - val_mse: 8.8673\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1212 - mse: 10.1212 - val_loss: 8.8449 - val_mse: 8.8449\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1626 - mse: 10.1626 - val_loss: 8.7394 - val_mse: 8.7394\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0748 - mse: 10.0748 - val_loss: 8.7478 - val_mse: 8.7478\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0042 - mse: 10.0042 - val_loss: 8.5939 - val_mse: 8.5939\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0265 - mse: 10.0265 - val_loss: 8.4859 - val_mse: 8.4859\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9758 - mse: 9.9758 - val_loss: 8.5695 - val_mse: 8.5695\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9248 - mse: 9.9248 - val_loss: 8.6488 - val_mse: 8.6488\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8710 - mse: 9.8710 - val_loss: 8.5529 - val_mse: 8.5529\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9126 - mse: 9.9126 - val_loss: 8.5587 - val_mse: 8.5587\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7935 - mse: 9.7935 - val_loss: 8.5659 - val_mse: 8.5659\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8905 - mse: 9.8905 - val_loss: 8.5325 - val_mse: 8.5325\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7477 - mse: 9.7477 - val_loss: 8.4848 - val_mse: 8.4848\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6792 - mse: 9.6792 - val_loss: 8.4543 - val_mse: 8.4543\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6416 - mse: 9.6416 - val_loss: 8.4658 - val_mse: 8.4658\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.5739 - mse: 9.5739 - val_loss: 8.4098 - val_mse: 8.4098\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6873 - mse: 9.6873 - val_loss: 8.4233 - val_mse: 8.4233\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5593 - mse: 9.5593 - val_loss: 8.3933 - val_mse: 8.3933\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4859 - mse: 9.4859 - val_loss: 8.3475 - val_mse: 8.3475\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5149 - mse: 9.5149 - val_loss: 8.3339 - val_mse: 8.3339\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4566 - mse: 9.4566 - val_loss: 8.0644 - val_mse: 8.0644\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4133 - mse: 9.4133 - val_loss: 8.0216 - val_mse: 8.0216\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3370 - mse: 9.3370 - val_loss: 7.9408 - val_mse: 7.9408\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3907 - mse: 9.3907 - val_loss: 8.1283 - val_mse: 8.1283\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4657 - mse: 9.4657 - val_loss: 8.1186 - val_mse: 8.1186\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2534 - mse: 9.2534 - val_loss: 8.0285 - val_mse: 8.0285\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3049 - mse: 9.3049 - val_loss: 7.9747 - val_mse: 7.9747\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1927 - mse: 9.1927 - val_loss: 8.0104 - val_mse: 8.0104\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1750 - mse: 9.1750 - val_loss: 7.9443 - val_mse: 7.9443\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1303 - mse: 9.1303 - val_loss: 8.1067 - val_mse: 8.1067\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3041 - mse: 9.3041 - val_loss: 8.1141 - val_mse: 8.1141\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1097 - mse: 9.1097 - val_loss: 8.1901 - val_mse: 8.1901\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1565 - mse: 9.1565 - val_loss: 8.0578 - val_mse: 8.0578\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0314 - mse: 9.0314 - val_loss: 7.9186 - val_mse: 7.9186\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0886 - mse: 9.0886 - val_loss: 7.8484 - val_mse: 7.8484\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9776 - mse: 8.9776 - val_loss: 7.8279 - val_mse: 7.8279\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.9207 - mse: 8.9207 - val_loss: 7.7890 - val_mse: 7.7890\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9858 - mse: 8.9858 - val_loss: 7.8338 - val_mse: 7.8338\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8843 - mse: 8.8843 - val_loss: 7.8087 - val_mse: 7.8087\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8738 - mse: 8.8738 - val_loss: 7.6755 - val_mse: 7.6755\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7646 - mse: 8.7646 - val_loss: 7.7829 - val_mse: 7.7829\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8147 - mse: 8.8147 - val_loss: 7.7562 - val_mse: 7.7562\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8365 - mse: 8.8365 - val_loss: 7.6951 - val_mse: 7.6951\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7052 - mse: 8.7052 - val_loss: 7.6914 - val_mse: 7.6914\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7010 - mse: 8.7010 - val_loss: 7.6483 - val_mse: 7.6483\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7561 - mse: 8.7561 - val_loss: 7.6258 - val_mse: 7.6258\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7469 - mse: 8.7469 - val_loss: 7.4552 - val_mse: 7.4552\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8024 - mse: 8.8024 - val_loss: 7.4551 - val_mse: 7.4551\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6650 - mse: 8.6650 - val_loss: 7.4845 - val_mse: 7.4845\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5508 - mse: 8.5508 - val_loss: 7.5830 - val_mse: 7.5830\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5321 - mse: 8.5321 - val_loss: 7.4929 - val_mse: 7.4929\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5127 - mse: 8.5127 - val_loss: 7.4794 - val_mse: 7.4794\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4964 - mse: 8.4964 - val_loss: 7.4469 - val_mse: 7.4469\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4650 - mse: 8.4650 - val_loss: 7.3174 - val_mse: 7.3174\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5166 - mse: 8.5166 - val_loss: 7.3905 - val_mse: 7.3905\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3606 - mse: 8.3606 - val_loss: 7.3926 - val_mse: 7.3926\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4347 - mse: 8.4347 - val_loss: 7.3161 - val_mse: 7.3161\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5234 - mse: 8.5234 - val_loss: 7.3598 - val_mse: 7.3598\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3337 - mse: 8.3337 - val_loss: 7.2944 - val_mse: 7.2944\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4370 - mse: 8.4370 - val_loss: 7.0618 - val_mse: 7.0618\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4101 - mse: 8.4101 - val_loss: 7.0810 - val_mse: 7.0810\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3485 - mse: 8.3485 - val_loss: 7.1072 - val_mse: 7.1072\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2233 - mse: 8.2233 - val_loss: 7.1408 - val_mse: 7.1408\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3321 - mse: 8.3321 - val_loss: 7.2087 - val_mse: 7.2087\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3846 - mse: 8.3846 - val_loss: 7.2476 - val_mse: 7.2476\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2328 - mse: 8.2328 - val_loss: 7.0804 - val_mse: 7.0804\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2244 - mse: 8.2244 - val_loss: 7.2083 - val_mse: 7.2083\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2522 - mse: 8.2522 - val_loss: 7.1153 - val_mse: 7.1153\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1055 - mse: 8.1055 - val_loss: 7.0364 - val_mse: 7.0364\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0964 - mse: 8.0964 - val_loss: 7.0364 - val_mse: 7.0364\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0245 - mse: 8.0245 - val_loss: 6.9878 - val_mse: 6.9878\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0812 - mse: 8.0812 - val_loss: 7.0826 - val_mse: 7.0826\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0014 - mse: 8.0014 - val_loss: 7.0483 - val_mse: 7.0483\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9278 - mse: 7.9278 - val_loss: 6.9684 - val_mse: 6.9684\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0433 - mse: 8.0433 - val_loss: 6.9074 - val_mse: 6.9074\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0926 - mse: 8.0926 - val_loss: 6.9606 - val_mse: 6.9606\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0954 - mse: 8.0954 - val_loss: 7.1341 - val_mse: 7.1341\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3762 - mse: 8.3762 - val_loss: 6.9677 - val_mse: 6.9677\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8452 - mse: 7.8452 - val_loss: 7.0183 - val_mse: 7.0183\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0073 - mse: 8.0073 - val_loss: 6.8267 - val_mse: 6.8267\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8206 - mse: 7.8206 - val_loss: 6.7598 - val_mse: 6.7598\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9249 - mse: 7.9249 - val_loss: 6.8319 - val_mse: 6.8319\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8605 - mse: 7.8605 - val_loss: 6.9845 - val_mse: 6.9845\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7724 - mse: 7.7724 - val_loss: 6.9657 - val_mse: 6.9657\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7980 - mse: 7.7980 - val_loss: 6.6921 - val_mse: 6.6921\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6213 - mse: 7.6213 - val_loss: 6.5747 - val_mse: 6.5747\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6730 - mse: 7.6730 - val_loss: 6.7260 - val_mse: 6.7260\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7098 - mse: 7.7098 - val_loss: 6.7589 - val_mse: 6.7589\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6156 - mse: 7.6156 - val_loss: 6.7403 - val_mse: 6.7403\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5999 - mse: 7.5999 - val_loss: 6.5357 - val_mse: 6.5357\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7071 - mse: 7.7071 - val_loss: 6.7079 - val_mse: 6.7079\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5594 - mse: 7.5594 - val_loss: 6.5033 - val_mse: 6.5033\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5364 - mse: 7.5364 - val_loss: 6.5503 - val_mse: 6.5503\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5259 - mse: 7.5259 - val_loss: 6.5355 - val_mse: 6.5355\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4794 - mse: 7.4794 - val_loss: 6.4930 - val_mse: 6.4930\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4452 - mse: 7.4452 - val_loss: 6.5761 - val_mse: 6.5761\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3862 - mse: 7.3862 - val_loss: 6.5326 - val_mse: 6.5326\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4272 - mse: 7.4272 - val_loss: 6.4662 - val_mse: 6.4662\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4367 - mse: 7.4367 - val_loss: 6.3784 - val_mse: 6.3784\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3653 - mse: 7.3653 - val_loss: 6.4594 - val_mse: 6.4594\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3702 - mse: 7.3702 - val_loss: 6.4343 - val_mse: 6.4343\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3491 - mse: 7.3491 - val_loss: 6.5964 - val_mse: 6.5964\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2794 - mse: 7.2794 - val_loss: 6.4264 - val_mse: 6.4264\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3437 - mse: 7.3437 - val_loss: 6.3309 - val_mse: 6.3309\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4394 - mse: 7.4394 - val_loss: 6.5213 - val_mse: 6.5213\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3537 - mse: 7.3537 - val_loss: 6.4920 - val_mse: 6.4920\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2261 - mse: 7.2261 - val_loss: 6.4247 - val_mse: 6.4247\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2242 - mse: 7.2242 - val_loss: 6.4603 - val_mse: 6.4603\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1731 - mse: 7.1731 - val_loss: 6.3535 - val_mse: 6.3535\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1460 - mse: 7.1460 - val_loss: 6.3694 - val_mse: 6.3694\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2002 - mse: 7.2002 - val_loss: 6.2366 - val_mse: 6.2366\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1693 - mse: 7.1693 - val_loss: 6.3980 - val_mse: 6.3980\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2156 - mse: 7.2156 - val_loss: 6.3089 - val_mse: 6.3089\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0591 - mse: 7.0591 - val_loss: 6.3772 - val_mse: 6.3772\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0628 - mse: 7.0628 - val_loss: 6.2361 - val_mse: 6.2361\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0667 - mse: 7.0667 - val_loss: 6.4023 - val_mse: 6.4023\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9990 - mse: 6.9990 - val_loss: 6.3714 - val_mse: 6.3714\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9812 - mse: 6.9812 - val_loss: 6.2706 - val_mse: 6.2706\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9753 - mse: 6.9753 - val_loss: 6.2128 - val_mse: 6.2128\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9669 - mse: 6.9669 - val_loss: 6.2964 - val_mse: 6.2964\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9529 - mse: 6.9529 - val_loss: 6.2029 - val_mse: 6.2029\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9814 - mse: 6.9814 - val_loss: 6.3978 - val_mse: 6.3978\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9217 - mse: 6.9217 - val_loss: 6.2138 - val_mse: 6.2138\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8798 - mse: 6.8798 - val_loss: 6.2698 - val_mse: 6.2698\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8472 - mse: 6.8472 - val_loss: 6.2248 - val_mse: 6.2248\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9384 - mse: 6.9384 - val_loss: 6.2186 - val_mse: 6.2186\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0821 - mse: 7.0821 - val_loss: 6.1129 - val_mse: 6.1129\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9196 - mse: 6.9196 - val_loss: 6.2232 - val_mse: 6.2232\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9392 - mse: 6.9392 - val_loss: 6.3060 - val_mse: 6.3060\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8852 - mse: 6.8852 - val_loss: 6.1302 - val_mse: 6.1302\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7294 - mse: 6.7294 - val_loss: 6.2306 - val_mse: 6.2306\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7510 - mse: 6.7510 - val_loss: 6.2357 - val_mse: 6.2357\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7042 - mse: 6.7042 - val_loss: 6.2915 - val_mse: 6.2915\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7100 - mse: 6.7100 - val_loss: 6.1397 - val_mse: 6.1397\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.6763 - mse: 6.6763 - val_loss: 6.1861 - val_mse: 6.1861\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6584 - mse: 6.6584 - val_loss: 6.1801 - val_mse: 6.1801\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7191 - mse: 6.7191 - val_loss: 6.1676 - val_mse: 6.1676\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7618 - mse: 6.7618 - val_loss: 6.0779 - val_mse: 6.0779\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8251 - mse: 6.8251 - val_loss: 6.1151 - val_mse: 6.1151\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6139 - mse: 6.6139 - val_loss: 6.0552 - val_mse: 6.0552\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7997 - mse: 6.7997 - val_loss: 6.2258 - val_mse: 6.2258\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5625 - mse: 6.5625 - val_loss: 6.1832 - val_mse: 6.1832\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6051 - mse: 6.6051 - val_loss: 6.1728 - val_mse: 6.1728\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5994 - mse: 6.5994 - val_loss: 6.3197 - val_mse: 6.3197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88b0a10490>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "21766ff8-e648-4d57-bf60-eefd88f44e5a"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "\n",
        "mse = np.square(errors)\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6f0b55-3d3a-4c85-b659-786a9ea2d524"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 22.2858 - mae: 22.2858 - val_loss: 20.8765 - val_mae: 20.8765\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 21.3877 - mae: 21.3877 - val_loss: 19.6570 - val_mae: 19.6570\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 19.7433 - mae: 19.7433 - val_loss: 17.3555 - val_mae: 17.3555\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.6870 - mae: 16.6870 - val_loss: 13.2803 - val_mae: 13.2803\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.6912 - mae: 11.6912 - val_loss: 7.1638 - val_mae: 7.1638\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0154 - mae: 7.0154 - val_loss: 5.7258 - val_mae: 5.7258\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1065 - mae: 6.1065 - val_loss: 4.2332 - val_mae: 4.2332\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8965 - mae: 4.8965 - val_loss: 3.5554 - val_mae: 3.5554\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.0528 - mae: 4.0528 - val_loss: 3.7228 - val_mae: 3.7228\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.6047 - mae: 3.6047 - val_loss: 3.1532 - val_mae: 3.1532\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.4218 - mae: 3.4218 - val_loss: 3.2733 - val_mae: 3.2733\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.2205 - mae: 3.2205 - val_loss: 3.1921 - val_mae: 3.1921\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.0550 - mae: 3.0550 - val_loss: 2.7914 - val_mae: 2.7914\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.9462 - mae: 2.9462 - val_loss: 2.8153 - val_mae: 2.8153\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.8386 - mae: 2.8386 - val_loss: 2.7099 - val_mae: 2.7099\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.7468 - mae: 2.7468 - val_loss: 2.7062 - val_mae: 2.7062\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6590 - mae: 2.6590 - val_loss: 2.5019 - val_mae: 2.5019\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.5541 - mae: 2.5541 - val_loss: 2.6849 - val_mae: 2.6849\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.5432 - mae: 2.5432 - val_loss: 2.4182 - val_mae: 2.4182\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.5094 - mae: 2.5094 - val_loss: 2.5236 - val_mae: 2.5236\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.4361 - mae: 2.4361 - val_loss: 2.4203 - val_mae: 2.4203\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4356 - mae: 2.4356 - val_loss: 2.3498 - val_mae: 2.3498\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4065 - mae: 2.4065 - val_loss: 2.3766 - val_mae: 2.3766\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2851 - mae: 2.2851 - val_loss: 2.2891 - val_mae: 2.2891\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2417 - mae: 2.2417 - val_loss: 2.3771 - val_mae: 2.3771\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2264 - mae: 2.2264 - val_loss: 2.2260 - val_mae: 2.2260\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1815 - mae: 2.1815 - val_loss: 2.3100 - val_mae: 2.3100\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1962 - mae: 2.1962 - val_loss: 2.2576 - val_mae: 2.2576\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1737 - mae: 2.1737 - val_loss: 2.2603 - val_mae: 2.2603\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1348 - mae: 2.1348 - val_loss: 2.2619 - val_mae: 2.2619\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1232 - mae: 2.1232 - val_loss: 2.2454 - val_mae: 2.2454\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1307 - mae: 2.1307 - val_loss: 2.2202 - val_mae: 2.2202\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1081 - mae: 2.1081 - val_loss: 2.2285 - val_mae: 2.2285\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0492 - mae: 2.0492 - val_loss: 2.2541 - val_mae: 2.2541\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0892 - mae: 2.0892 - val_loss: 2.2216 - val_mae: 2.2216\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0608 - mae: 2.0608 - val_loss: 2.2513 - val_mae: 2.2513\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0554 - mae: 2.0554 - val_loss: 2.2328 - val_mae: 2.2328\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0495 - mae: 2.0495 - val_loss: 2.2624 - val_mae: 2.2624\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0269 - mae: 2.0269 - val_loss: 2.1777 - val_mae: 2.1777\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0504 - mae: 2.0504 - val_loss: 2.2052 - val_mae: 2.2052\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9804 - mae: 1.9804 - val_loss: 2.1811 - val_mae: 2.1811\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9579 - mae: 1.9579 - val_loss: 2.1999 - val_mae: 2.1999\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9443 - mae: 1.9443 - val_loss: 2.2211 - val_mae: 2.2211\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9556 - mae: 1.9556 - val_loss: 2.2493 - val_mae: 2.2493\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9494 - mae: 1.9494 - val_loss: 2.2113 - val_mae: 2.2113\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9646 - mae: 1.9646 - val_loss: 2.2714 - val_mae: 2.2714\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9978 - mae: 1.9978 - val_loss: 2.1933 - val_mae: 2.1933\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8902 - mae: 1.8902 - val_loss: 2.2357 - val_mae: 2.2357\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8947 - mae: 1.8947 - val_loss: 2.2887 - val_mae: 2.2887\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8826 - mae: 1.8826 - val_loss: 2.2360 - val_mae: 2.2360\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8391 - mae: 1.8391 - val_loss: 2.2619 - val_mae: 2.2619\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8438 - mae: 1.8438 - val_loss: 2.1721 - val_mae: 2.1721\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8302 - mae: 1.8302 - val_loss: 2.1713 - val_mae: 2.1713\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8047 - mae: 1.8047 - val_loss: 2.1603 - val_mae: 2.1603\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8257 - mae: 1.8257 - val_loss: 2.1708 - val_mae: 2.1708\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7873 - mae: 1.7873 - val_loss: 2.2358 - val_mae: 2.2358\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8056 - mae: 1.8056 - val_loss: 2.2541 - val_mae: 2.2541\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7554 - mae: 1.7554 - val_loss: 2.2901 - val_mae: 2.2901\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8219 - mae: 1.8219 - val_loss: 2.2385 - val_mae: 2.2385\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7565 - mae: 1.7565 - val_loss: 2.1363 - val_mae: 2.1363\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7723 - mae: 1.7723 - val_loss: 2.3500 - val_mae: 2.3500\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7160 - mae: 1.7160 - val_loss: 2.2644 - val_mae: 2.2644\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7084 - mae: 1.7084 - val_loss: 2.2356 - val_mae: 2.2356\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7259 - mae: 1.7259 - val_loss: 2.3076 - val_mae: 2.3076\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7442 - mae: 1.7442 - val_loss: 2.1808 - val_mae: 2.1808\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7279 - mae: 1.7279 - val_loss: 2.1789 - val_mae: 2.1789\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6787 - mae: 1.6787 - val_loss: 2.2614 - val_mae: 2.2614\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6661 - mae: 1.6661 - val_loss: 2.2423 - val_mae: 2.2423\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6382 - mae: 1.6382 - val_loss: 2.2461 - val_mae: 2.2461\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6474 - mae: 1.6474 - val_loss: 2.2284 - val_mae: 2.2284\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6595 - mae: 1.6595 - val_loss: 2.2637 - val_mae: 2.2637\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6318 - mae: 1.6318 - val_loss: 2.1838 - val_mae: 2.1838\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6737 - mae: 1.6737 - val_loss: 2.2482 - val_mae: 2.2482\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6215 - mae: 1.6215 - val_loss: 2.1956 - val_mae: 2.1956\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6017 - mae: 1.6017 - val_loss: 2.3103 - val_mae: 2.3103\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6113 - mae: 1.6113 - val_loss: 2.2049 - val_mae: 2.2049\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5783 - mae: 1.5783 - val_loss: 2.1643 - val_mae: 2.1643\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5947 - mae: 1.5947 - val_loss: 2.2945 - val_mae: 2.2945\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5719 - mae: 1.5719 - val_loss: 2.1309 - val_mae: 2.1309\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6631 - mae: 1.6631 - val_loss: 2.2347 - val_mae: 2.2347\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6006 - mae: 1.6006 - val_loss: 2.3082 - val_mae: 2.3082\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5387 - mae: 1.5387 - val_loss: 2.2324 - val_mae: 2.2324\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5163 - mae: 1.5163 - val_loss: 2.2606 - val_mae: 2.2606\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5474 - mae: 1.5474 - val_loss: 2.2110 - val_mae: 2.2110\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5496 - mae: 1.5496 - val_loss: 2.2039 - val_mae: 2.2039\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5218 - mae: 1.5218 - val_loss: 2.2026 - val_mae: 2.2026\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5496 - mae: 1.5496 - val_loss: 2.2721 - val_mae: 2.2721\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5465 - mae: 1.5465 - val_loss: 2.2555 - val_mae: 2.2555\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5153 - mae: 1.5153 - val_loss: 2.2424 - val_mae: 2.2424\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5278 - mae: 1.5278 - val_loss: 2.1543 - val_mae: 2.1543\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4879 - mae: 1.4879 - val_loss: 2.2305 - val_mae: 2.2305\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5038 - mae: 1.5038 - val_loss: 2.2289 - val_mae: 2.2289\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4840 - mae: 1.4840 - val_loss: 2.2822 - val_mae: 2.2822\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4654 - mae: 1.4654 - val_loss: 2.1710 - val_mae: 2.1710\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4657 - mae: 1.4657 - val_loss: 2.2665 - val_mae: 2.2665\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4546 - mae: 1.4546 - val_loss: 2.2199 - val_mae: 2.2199\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4811 - mae: 1.4811 - val_loss: 2.3080 - val_mae: 2.3080\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4531 - mae: 1.4531 - val_loss: 2.2207 - val_mae: 2.2207\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4476 - mae: 1.4476 - val_loss: 2.2232 - val_mae: 2.2232\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4337 - mae: 1.4337 - val_loss: 2.2532 - val_mae: 2.2532\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4591 - mae: 1.4591 - val_loss: 2.2605 - val_mae: 2.2605\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5681 - mae: 1.5681 - val_loss: 2.1804 - val_mae: 2.1804\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4604 - mae: 1.4604 - val_loss: 2.1237 - val_mae: 2.1237\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5102 - mae: 1.5102 - val_loss: 2.1502 - val_mae: 2.1502\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3996 - mae: 1.3996 - val_loss: 2.3085 - val_mae: 2.3085\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4517 - mae: 1.4517 - val_loss: 2.1836 - val_mae: 2.1836\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3982 - mae: 1.3982 - val_loss: 2.2083 - val_mae: 2.2083\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3607 - mae: 1.3607 - val_loss: 2.2516 - val_mae: 2.2516\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4076 - mae: 1.4076 - val_loss: 2.2503 - val_mae: 2.2503\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3784 - mae: 1.3784 - val_loss: 2.2070 - val_mae: 2.2070\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3453 - mae: 1.3453 - val_loss: 2.2128 - val_mae: 2.2128\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4109 - mae: 1.4109 - val_loss: 2.1782 - val_mae: 2.1782\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3759 - mae: 1.3759 - val_loss: 2.2229 - val_mae: 2.2229\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3657 - mae: 1.3657 - val_loss: 2.2658 - val_mae: 2.2658\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4194 - mae: 1.4194 - val_loss: 2.2010 - val_mae: 2.2010\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3757 - mae: 1.3757 - val_loss: 2.1607 - val_mae: 2.1607\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3759 - mae: 1.3759 - val_loss: 2.1753 - val_mae: 2.1753\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4098 - mae: 1.4098 - val_loss: 2.1708 - val_mae: 2.1708\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4313 - mae: 1.4313 - val_loss: 2.1815 - val_mae: 2.1815\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3927 - mae: 1.3927 - val_loss: 2.2038 - val_mae: 2.2038\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3691 - mae: 1.3691 - val_loss: 2.1639 - val_mae: 2.1639\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3755 - mae: 1.3755 - val_loss: 2.2304 - val_mae: 2.2304\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4292 - mae: 1.4292 - val_loss: 2.1215 - val_mae: 2.1215\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3384 - mae: 1.3384 - val_loss: 2.1920 - val_mae: 2.1920\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3051 - mae: 1.3051 - val_loss: 2.1723 - val_mae: 2.1723\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3198 - mae: 1.3198 - val_loss: 2.2118 - val_mae: 2.2118\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3101 - mae: 1.3101 - val_loss: 2.1831 - val_mae: 2.1831\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2785 - mae: 1.2785 - val_loss: 2.1711 - val_mae: 2.1711\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3028 - mae: 1.3028 - val_loss: 2.1372 - val_mae: 2.1372\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2900 - mae: 1.2900 - val_loss: 2.1398 - val_mae: 2.1398\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3008 - mae: 1.3008 - val_loss: 2.1082 - val_mae: 2.1082\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3466 - mae: 1.3466 - val_loss: 2.1249 - val_mae: 2.1249\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2713 - mae: 1.2713 - val_loss: 2.2223 - val_mae: 2.2223\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3457 - mae: 1.3457 - val_loss: 2.1879 - val_mae: 2.1879\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3012 - mae: 1.3012 - val_loss: 2.1571 - val_mae: 2.1571\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2815 - mae: 1.2815 - val_loss: 2.1244 - val_mae: 2.1244\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2455 - mae: 1.2455 - val_loss: 2.1479 - val_mae: 2.1479\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3009 - mae: 1.3009 - val_loss: 2.1455 - val_mae: 2.1455\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2506 - mae: 1.2506 - val_loss: 2.1530 - val_mae: 2.1530\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2345 - mae: 1.2345 - val_loss: 2.1675 - val_mae: 2.1675\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2281 - mae: 1.2281 - val_loss: 2.1059 - val_mae: 2.1059\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2736 - mae: 1.2736 - val_loss: 2.1730 - val_mae: 2.1730\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2724 - mae: 1.2724 - val_loss: 2.0998 - val_mae: 2.0998\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2300 - mae: 1.2300 - val_loss: 2.1459 - val_mae: 2.1459\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2342 - mae: 1.2342 - val_loss: 2.1521 - val_mae: 2.1521\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2516 - mae: 1.2516 - val_loss: 2.1673 - val_mae: 2.1673\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2371 - mae: 1.2371 - val_loss: 2.1528 - val_mae: 2.1528\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2258 - mae: 1.2258 - val_loss: 2.1087 - val_mae: 2.1087\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1834 - mae: 1.1834 - val_loss: 2.1106 - val_mae: 2.1106\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2221 - mae: 1.2221 - val_loss: 2.1539 - val_mae: 2.1539\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2144 - mae: 1.2144 - val_loss: 2.1024 - val_mae: 2.1024\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2181 - mae: 1.2181 - val_loss: 2.1117 - val_mae: 2.1117\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1890 - mae: 1.1890 - val_loss: 2.1182 - val_mae: 2.1182\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1560 - mae: 1.1560 - val_loss: 2.1081 - val_mae: 2.1081\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1663 - mae: 1.1663 - val_loss: 2.1348 - val_mae: 2.1348\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1480 - mae: 1.1480 - val_loss: 2.1873 - val_mae: 2.1873\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1773 - mae: 1.1773 - val_loss: 2.1477 - val_mae: 2.1477\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1364 - mae: 1.1364 - val_loss: 2.1550 - val_mae: 2.1550\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1527 - mae: 1.1527 - val_loss: 2.1238 - val_mae: 2.1238\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1277 - mae: 1.1277 - val_loss: 2.1612 - val_mae: 2.1612\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1192 - mae: 1.1192 - val_loss: 2.1461 - val_mae: 2.1461\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1384 - mae: 1.1384 - val_loss: 2.1768 - val_mae: 2.1768\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1655 - mae: 1.1655 - val_loss: 2.1830 - val_mae: 2.1830\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1484 - mae: 1.1484 - val_loss: 2.1572 - val_mae: 2.1572\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1496 - mae: 1.1496 - val_loss: 2.1506 - val_mae: 2.1506\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1114 - mae: 1.1114 - val_loss: 2.0841 - val_mae: 2.0841\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0892 - mae: 1.0892 - val_loss: 2.1388 - val_mae: 2.1388\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1159 - mae: 1.1159 - val_loss: 2.1013 - val_mae: 2.1013\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1625 - mae: 1.1625 - val_loss: 2.0731 - val_mae: 2.0731\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1234 - mae: 1.1234 - val_loss: 2.1498 - val_mae: 2.1498\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0923 - mae: 1.0923 - val_loss: 2.0805 - val_mae: 2.0805\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1330 - mae: 1.1330 - val_loss: 2.0861 - val_mae: 2.0861\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0824 - mae: 1.0824 - val_loss: 2.1107 - val_mae: 2.1107\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1127 - mae: 1.1127 - val_loss: 2.1913 - val_mae: 2.1913\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1300 - mae: 1.1300 - val_loss: 2.1152 - val_mae: 2.1152\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0826 - mae: 1.0826 - val_loss: 2.1172 - val_mae: 2.1172\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1150 - mae: 1.1150 - val_loss: 2.1254 - val_mae: 2.1254\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1134 - mae: 1.1134 - val_loss: 2.1346 - val_mae: 2.1346\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0756 - mae: 1.0756 - val_loss: 2.0703 - val_mae: 2.0703\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1304 - mae: 1.1304 - val_loss: 2.1599 - val_mae: 2.1599\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1277 - mae: 1.1277 - val_loss: 2.1810 - val_mae: 2.1810\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1061 - mae: 1.1061 - val_loss: 2.0679 - val_mae: 2.0679\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0524 - mae: 1.0524 - val_loss: 2.1177 - val_mae: 2.1177\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0776 - mae: 1.0776 - val_loss: 2.1296 - val_mae: 2.1296\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1093 - mae: 1.1093 - val_loss: 2.0857 - val_mae: 2.0857\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0826 - mae: 1.0826 - val_loss: 2.0562 - val_mae: 2.0562\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0931 - mae: 1.0931 - val_loss: 2.1056 - val_mae: 2.1056\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0498 - mae: 1.0498 - val_loss: 2.1065 - val_mae: 2.1065\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0209 - mae: 1.0209 - val_loss: 2.0926 - val_mae: 2.0926\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0396 - mae: 1.0396 - val_loss: 2.0495 - val_mae: 2.0495\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0279 - mae: 1.0279 - val_loss: 2.1479 - val_mae: 2.1479\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0358 - mae: 1.0358 - val_loss: 2.0927 - val_mae: 2.0927\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0255 - mae: 1.0255 - val_loss: 2.1260 - val_mae: 2.1260\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0477 - mae: 1.0477 - val_loss: 2.1459 - val_mae: 2.1459\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0948 - mae: 1.0948 - val_loss: 2.0618 - val_mae: 2.0618\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1365 - mae: 1.1365 - val_loss: 2.0845 - val_mae: 2.0845\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1505 - mae: 1.1505 - val_loss: 2.0951 - val_mae: 2.0951\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0911 - mae: 1.0911 - val_loss: 2.0599 - val_mae: 2.0599\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0068 - mae: 1.0068 - val_loss: 2.1127 - val_mae: 2.1127\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0036 - mae: 1.0036 - val_loss: 2.1016 - val_mae: 2.1016\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0394 - mae: 1.0394 - val_loss: 2.0956 - val_mae: 2.0956\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0267 - mae: 1.0267 - val_loss: 2.1121 - val_mae: 2.1121\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0536 - mae: 1.0536 - val_loss: 2.0934 - val_mae: 2.0934\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0334 - mae: 1.0334 - val_loss: 2.1205 - val_mae: 2.1205\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0263 - mae: 1.0263 - val_loss: 2.1001 - val_mae: 2.1001\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9789 - mae: 0.9789 - val_loss: 2.1040 - val_mae: 2.1040\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0183 - mae: 1.0183 - val_loss: 2.1426 - val_mae: 2.1426\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0654 - mae: 1.0654 - val_loss: 2.1142 - val_mae: 2.1142\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0392 - mae: 1.0392 - val_loss: 2.0851 - val_mae: 2.0851\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0201 - mae: 1.0201 - val_loss: 2.0717 - val_mae: 2.0717\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0033 - mae: 1.0033 - val_loss: 2.0627 - val_mae: 2.0627\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0473 - mae: 1.0473 - val_loss: 2.0671 - val_mae: 2.0671\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0630 - mae: 1.0630 - val_loss: 2.0826 - val_mae: 2.0826\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1337 - mae: 1.1337 - val_loss: 2.0252 - val_mae: 2.0252\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1301 - mae: 1.1301 - val_loss: 2.0496 - val_mae: 2.0496\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0335 - mae: 1.0335 - val_loss: 2.0677 - val_mae: 2.0677\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0763 - mae: 1.0763 - val_loss: 2.0145 - val_mae: 2.0145\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9995 - mae: 0.9995 - val_loss: 2.0680 - val_mae: 2.0680\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9767 - mae: 0.9767 - val_loss: 2.1490 - val_mae: 2.1490\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1572 - mae: 1.1572 - val_loss: 2.1882 - val_mae: 2.1882\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0455 - mae: 1.0455 - val_loss: 1.9966 - val_mae: 1.9966\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9620 - mae: 0.9620 - val_loss: 2.0645 - val_mae: 2.0645\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9715 - mae: 0.9715 - val_loss: 2.0949 - val_mae: 2.0949\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9714 - mae: 0.9714 - val_loss: 1.9812 - val_mae: 1.9812\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0145 - mae: 1.0145 - val_loss: 1.9415 - val_mae: 1.9415\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9446 - mae: 0.9446 - val_loss: 2.0313 - val_mae: 2.0313\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9304 - mae: 0.9304 - val_loss: 1.9629 - val_mae: 1.9629\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9317 - mae: 0.9317 - val_loss: 2.0201 - val_mae: 2.0201\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9331 - mae: 0.9331 - val_loss: 2.0448 - val_mae: 2.0448\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9443 - mae: 0.9443 - val_loss: 2.0014 - val_mae: 2.0014\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9361 - mae: 0.9361 - val_loss: 1.9926 - val_mae: 1.9926\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9418 - mae: 0.9418 - val_loss: 1.9929 - val_mae: 1.9929\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9280 - mae: 0.9280 - val_loss: 1.9944 - val_mae: 1.9944\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9186 - mae: 0.9186 - val_loss: 1.9465 - val_mae: 1.9465\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9553 - mae: 0.9553 - val_loss: 2.0097 - val_mae: 2.0097\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9925 - mae: 0.9925 - val_loss: 2.0259 - val_mae: 2.0259\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9681 - mae: 0.9681 - val_loss: 2.0015 - val_mae: 2.0015\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9975 - mae: 0.9975 - val_loss: 2.0136 - val_mae: 2.0136\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9747 - mae: 0.9747 - val_loss: 2.0114 - val_mae: 2.0114\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9402 - mae: 0.9402 - val_loss: 2.0024 - val_mae: 2.0024\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9670 - mae: 0.9670 - val_loss: 2.1159 - val_mae: 2.1159\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0100 - mae: 1.0100 - val_loss: 2.0064 - val_mae: 2.0064\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9413 - mae: 0.9413 - val_loss: 1.8883 - val_mae: 1.8883\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9668 - mae: 0.9668 - val_loss: 1.9716 - val_mae: 1.9716\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9301 - mae: 0.9301 - val_loss: 2.0603 - val_mae: 2.0603\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9118 - mae: 0.9118 - val_loss: 2.0218 - val_mae: 2.0218\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8854 - mae: 0.8854 - val_loss: 2.0279 - val_mae: 2.0279\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9139 - mae: 0.9139 - val_loss: 1.9929 - val_mae: 1.9929\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9075 - mae: 0.9075 - val_loss: 1.9942 - val_mae: 1.9942\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9030 - mae: 0.9030 - val_loss: 1.9147 - val_mae: 1.9147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f884612a110>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "9c5b8f53-0e21-47ff-dd42-12e36777d224"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "mae = abs(errors)\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99417047-0c9f-4f3a-ab83-f119d450d6cd"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 8.5915 - mean_squared_logarithmic_error: 8.5915 - val_loss: 7.2481 - val_mean_squared_logarithmic_error: 7.2481\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3284 - mean_squared_logarithmic_error: 6.3284 - val_loss: 4.9945 - val_mean_squared_logarithmic_error: 4.9945\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.2349 - mean_squared_logarithmic_error: 4.2349 - val_loss: 3.1649 - val_mean_squared_logarithmic_error: 3.1649\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6374 - mean_squared_logarithmic_error: 2.6374 - val_loss: 1.8548 - val_mean_squared_logarithmic_error: 1.8548\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5428 - mean_squared_logarithmic_error: 1.5428 - val_loss: 1.0351 - val_mean_squared_logarithmic_error: 1.0351\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8763 - mean_squared_logarithmic_error: 0.8763 - val_loss: 0.5497 - val_mean_squared_logarithmic_error: 0.5497\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4831 - mean_squared_logarithmic_error: 0.4831 - val_loss: 0.2931 - val_mean_squared_logarithmic_error: 0.2931\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2815 - mean_squared_logarithmic_error: 0.2815 - val_loss: 0.1793 - val_mean_squared_logarithmic_error: 0.1793\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1952 - mean_squared_logarithmic_error: 0.1952 - val_loss: 0.1309 - val_mean_squared_logarithmic_error: 0.1309\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1519 - mean_squared_logarithmic_error: 0.1519 - val_loss: 0.1078 - val_mean_squared_logarithmic_error: 0.1078\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1299 - mean_squared_logarithmic_error: 0.1299 - val_loss: 0.0938 - val_mean_squared_logarithmic_error: 0.0938\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1156 - mean_squared_logarithmic_error: 0.1156 - val_loss: 0.0834 - val_mean_squared_logarithmic_error: 0.0834\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1043 - mean_squared_logarithmic_error: 0.1043 - val_loss: 0.0752 - val_mean_squared_logarithmic_error: 0.0752\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0954 - mean_squared_logarithmic_error: 0.0954 - val_loss: 0.0679 - val_mean_squared_logarithmic_error: 0.0679\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0872 - mean_squared_logarithmic_error: 0.0872 - val_loss: 0.0618 - val_mean_squared_logarithmic_error: 0.0618\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0803 - mean_squared_logarithmic_error: 0.0803 - val_loss: 0.0560 - val_mean_squared_logarithmic_error: 0.0560\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0739 - mean_squared_logarithmic_error: 0.0739 - val_loss: 0.0512 - val_mean_squared_logarithmic_error: 0.0512\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_logarithmic_error: 0.0683 - val_loss: 0.0471 - val_mean_squared_logarithmic_error: 0.0471\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0632 - mean_squared_logarithmic_error: 0.0632 - val_loss: 0.0433 - val_mean_squared_logarithmic_error: 0.0433\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0589 - mean_squared_logarithmic_error: 0.0589 - val_loss: 0.0402 - val_mean_squared_logarithmic_error: 0.0402\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0548 - mean_squared_logarithmic_error: 0.0548 - val_loss: 0.0378 - val_mean_squared_logarithmic_error: 0.0378\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0513 - mean_squared_logarithmic_error: 0.0513 - val_loss: 0.0358 - val_mean_squared_logarithmic_error: 0.0358\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0484 - mean_squared_logarithmic_error: 0.0484 - val_loss: 0.0343 - val_mean_squared_logarithmic_error: 0.0343\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0455 - mean_squared_logarithmic_error: 0.0455 - val_loss: 0.0329 - val_mean_squared_logarithmic_error: 0.0329\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0433 - mean_squared_logarithmic_error: 0.0433 - val_loss: 0.0317 - val_mean_squared_logarithmic_error: 0.0317\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0413 - mean_squared_logarithmic_error: 0.0413 - val_loss: 0.0303 - val_mean_squared_logarithmic_error: 0.0303\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0395 - mean_squared_logarithmic_error: 0.0395 - val_loss: 0.0296 - val_mean_squared_logarithmic_error: 0.0296\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0378 - mean_squared_logarithmic_error: 0.0378 - val_loss: 0.0289 - val_mean_squared_logarithmic_error: 0.0289\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0364 - mean_squared_logarithmic_error: 0.0364 - val_loss: 0.0286 - val_mean_squared_logarithmic_error: 0.0286\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0352 - mean_squared_logarithmic_error: 0.0352 - val_loss: 0.0281 - val_mean_squared_logarithmic_error: 0.0281\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0341 - mean_squared_logarithmic_error: 0.0341 - val_loss: 0.0279 - val_mean_squared_logarithmic_error: 0.0279\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0333 - mean_squared_logarithmic_error: 0.0333 - val_loss: 0.0271 - val_mean_squared_logarithmic_error: 0.0271\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0323 - mean_squared_logarithmic_error: 0.0323 - val_loss: 0.0273 - val_mean_squared_logarithmic_error: 0.0273\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0313 - mean_squared_logarithmic_error: 0.0313 - val_loss: 0.0269 - val_mean_squared_logarithmic_error: 0.0269\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0306 - mean_squared_logarithmic_error: 0.0306 - val_loss: 0.0268 - val_mean_squared_logarithmic_error: 0.0268\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0298 - mean_squared_logarithmic_error: 0.0298 - val_loss: 0.0262 - val_mean_squared_logarithmic_error: 0.0262\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0292 - mean_squared_logarithmic_error: 0.0292 - val_loss: 0.0260 - val_mean_squared_logarithmic_error: 0.0260\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0285 - mean_squared_logarithmic_error: 0.0285 - val_loss: 0.0259 - val_mean_squared_logarithmic_error: 0.0259\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0281 - mean_squared_logarithmic_error: 0.0281 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0276 - mean_squared_logarithmic_error: 0.0276 - val_loss: 0.0254 - val_mean_squared_logarithmic_error: 0.0254\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0250 - val_mean_squared_logarithmic_error: 0.0250\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0267 - mean_squared_logarithmic_error: 0.0267 - val_loss: 0.0252 - val_mean_squared_logarithmic_error: 0.0252\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0262 - mean_squared_logarithmic_error: 0.0262 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0238 - val_mean_squared_logarithmic_error: 0.0238\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0254 - mean_squared_logarithmic_error: 0.0254 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0250 - mean_squared_logarithmic_error: 0.0250 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0245 - mean_squared_logarithmic_error: 0.0245 - val_loss: 0.0232 - val_mean_squared_logarithmic_error: 0.0232\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0242 - mean_squared_logarithmic_error: 0.0242 - val_loss: 0.0231 - val_mean_squared_logarithmic_error: 0.0231\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - mean_squared_logarithmic_error: 0.0239 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - mean_squared_logarithmic_error: 0.0235 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0233 - mean_squared_logarithmic_error: 0.0233 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - mean_squared_logarithmic_error: 0.0230 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0227 - mean_squared_logarithmic_error: 0.0227 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - mean_squared_logarithmic_error: 0.0230 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0222 - mean_squared_logarithmic_error: 0.0222 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - mean_squared_logarithmic_error: 0.0219 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0213 - mean_squared_logarithmic_error: 0.0213 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0211 - mean_squared_logarithmic_error: 0.0211 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0208 - mean_squared_logarithmic_error: 0.0208 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - mean_squared_logarithmic_error: 0.0204 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - mean_squared_logarithmic_error: 0.0200 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0209 - val_mean_squared_logarithmic_error: 0.0209\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0152 - mean_squared_logarithmic_error: 0.0152 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - mean_squared_logarithmic_error: 0.0148 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0146 - mean_squared_logarithmic_error: 0.0146 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - mean_squared_logarithmic_error: 0.0142 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - mean_squared_logarithmic_error: 0.0141 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - mean_squared_logarithmic_error: 0.0140 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - mean_squared_logarithmic_error: 0.0139 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - mean_squared_logarithmic_error: 0.0142 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - mean_squared_logarithmic_error: 0.0139 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - mean_squared_logarithmic_error: 0.0137 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0136 - mean_squared_logarithmic_error: 0.0136 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - mean_squared_logarithmic_error: 0.0139 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - mean_squared_logarithmic_error: 0.0138 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - mean_squared_logarithmic_error: 0.0134 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - mean_squared_logarithmic_error: 0.0135 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - mean_squared_logarithmic_error: 0.0133 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - mean_squared_logarithmic_error: 0.0133 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - mean_squared_logarithmic_error: 0.0133 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - mean_squared_logarithmic_error: 0.0135 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - mean_squared_logarithmic_error: 0.0131 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - mean_squared_logarithmic_error: 0.0130 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - mean_squared_logarithmic_error: 0.0131 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - mean_squared_logarithmic_error: 0.0129 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0129 - mean_squared_logarithmic_error: 0.0129 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0127 - mean_squared_logarithmic_error: 0.0127 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88460fa650>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "2851b09d-7b85-4d1c-b46b-15fd1a6d6021"
      },
      "source": [
        "actual_outputs = np.arange(0,51) + 1\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n) + 1\n",
        "\n",
        "msle = np.square(np.log(actual_outputs/estimated_outputs))\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual outputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 4\n",
        "\n",
        "This is because both y and y^ can be 0, and log(0) is undefined, so to prevent this we add 1 so that even if y or y^ are 0, we are calculating log(1) which equals 0."
      ],
      "metadata": {
        "id": "JaDHVCPza4wA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 5\n",
        "\n",
        "All three loss functions are minimized by the model in order to reach greater accuracy. In terms of magnitude, log loss function returns the lowest values for a larger difference in predicted and true values. Log loss also only cares about percentage difference, so MSE and MAE will give big difference if the values are big, but the percentage difference is same. "
      ],
      "metadata": {
        "id": "VQIx1LlEcYxg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n",
        "\n",
        "\\<*Type your answer here*\\>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a703c1d2-b528-4d6a-8aae-dd06c8e6a895"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= tf.keras.losses.MeanAbsolutePercentageError(),\n",
        "              metrics=['mean_absolute_percentage_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 14ms/step - loss: 97.3524 - mean_absolute_percentage_error: 97.3524 - val_loss: 94.5886 - val_mean_absolute_percentage_error: 94.5886\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 91.5772 - mean_absolute_percentage_error: 91.5772 - val_loss: 86.9128 - val_mean_absolute_percentage_error: 86.9128\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 82.4271 - mean_absolute_percentage_error: 82.4271 - val_loss: 76.5555 - val_mean_absolute_percentage_error: 76.5555\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 71.1417 - mean_absolute_percentage_error: 71.1417 - val_loss: 67.1204 - val_mean_absolute_percentage_error: 67.1204\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 60.5990 - mean_absolute_percentage_error: 60.5990 - val_loss: 55.5800 - val_mean_absolute_percentage_error: 55.5800\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 46.4457 - mean_absolute_percentage_error: 46.4457 - val_loss: 35.2863 - val_mean_absolute_percentage_error: 35.2863\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.2619 - mean_absolute_percentage_error: 31.2619 - val_loss: 26.0135 - val_mean_absolute_percentage_error: 26.0135\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 26.0934 - mean_absolute_percentage_error: 26.0934 - val_loss: 21.3563 - val_mean_absolute_percentage_error: 21.3563\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 21.5697 - mean_absolute_percentage_error: 21.5697 - val_loss: 16.5863 - val_mean_absolute_percentage_error: 16.5863\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.7811 - mean_absolute_percentage_error: 17.7811 - val_loss: 15.7041 - val_mean_absolute_percentage_error: 15.7041\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.7764 - mean_absolute_percentage_error: 15.7764 - val_loss: 15.1441 - val_mean_absolute_percentage_error: 15.1441\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.7029 - mean_absolute_percentage_error: 14.7029 - val_loss: 14.4471 - val_mean_absolute_percentage_error: 14.4471\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.9448 - mean_absolute_percentage_error: 13.9448 - val_loss: 13.7066 - val_mean_absolute_percentage_error: 13.7066\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.1849 - mean_absolute_percentage_error: 13.1849 - val_loss: 13.4002 - val_mean_absolute_percentage_error: 13.4002\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.7841 - mean_absolute_percentage_error: 12.7841 - val_loss: 13.2549 - val_mean_absolute_percentage_error: 13.2549\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.2276 - mean_absolute_percentage_error: 12.2276 - val_loss: 12.8198 - val_mean_absolute_percentage_error: 12.8198\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.8268 - mean_absolute_percentage_error: 11.8268 - val_loss: 12.3006 - val_mean_absolute_percentage_error: 12.3006\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3401 - mean_absolute_percentage_error: 11.3401 - val_loss: 13.1660 - val_mean_absolute_percentage_error: 13.1660\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1768 - mean_absolute_percentage_error: 11.1768 - val_loss: 12.6064 - val_mean_absolute_percentage_error: 12.6064\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9323 - mean_absolute_percentage_error: 10.9323 - val_loss: 12.4263 - val_mean_absolute_percentage_error: 12.4263\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6086 - mean_absolute_percentage_error: 10.6086 - val_loss: 12.4354 - val_mean_absolute_percentage_error: 12.4354\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5769 - mean_absolute_percentage_error: 10.5769 - val_loss: 12.4973 - val_mean_absolute_percentage_error: 12.4973\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6635 - mean_absolute_percentage_error: 10.6635 - val_loss: 11.9493 - val_mean_absolute_percentage_error: 11.9493\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4981 - mean_absolute_percentage_error: 10.4981 - val_loss: 11.7646 - val_mean_absolute_percentage_error: 11.7646\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0698 - mean_absolute_percentage_error: 10.0698 - val_loss: 11.5593 - val_mean_absolute_percentage_error: 11.5593\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9762 - mean_absolute_percentage_error: 9.9762 - val_loss: 12.2780 - val_mean_absolute_percentage_error: 12.2780\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8994 - mean_absolute_percentage_error: 9.8994 - val_loss: 12.1119 - val_mean_absolute_percentage_error: 12.1119\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8120 - mean_absolute_percentage_error: 9.8120 - val_loss: 11.4369 - val_mean_absolute_percentage_error: 11.4369\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.6559 - mean_absolute_percentage_error: 9.6559 - val_loss: 11.3214 - val_mean_absolute_percentage_error: 11.3214\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6126 - mean_absolute_percentage_error: 9.6126 - val_loss: 11.6564 - val_mean_absolute_percentage_error: 11.6564\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4705 - mean_absolute_percentage_error: 9.4705 - val_loss: 11.3851 - val_mean_absolute_percentage_error: 11.3851\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5436 - mean_absolute_percentage_error: 9.5436 - val_loss: 11.6158 - val_mean_absolute_percentage_error: 11.6158\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3231 - mean_absolute_percentage_error: 9.3231 - val_loss: 11.6972 - val_mean_absolute_percentage_error: 11.6972\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3794 - mean_absolute_percentage_error: 9.3794 - val_loss: 11.2857 - val_mean_absolute_percentage_error: 11.2857\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3468 - mean_absolute_percentage_error: 9.3468 - val_loss: 10.9891 - val_mean_absolute_percentage_error: 10.9891\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3741 - mean_absolute_percentage_error: 9.3741 - val_loss: 11.6733 - val_mean_absolute_percentage_error: 11.6733\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1482 - mean_absolute_percentage_error: 9.1482 - val_loss: 11.2155 - val_mean_absolute_percentage_error: 11.2155\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1329 - mean_absolute_percentage_error: 9.1329 - val_loss: 10.4174 - val_mean_absolute_percentage_error: 10.4174\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1457 - mean_absolute_percentage_error: 9.1457 - val_loss: 11.6461 - val_mean_absolute_percentage_error: 11.6461\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.9538 - mean_absolute_percentage_error: 8.9538 - val_loss: 11.4159 - val_mean_absolute_percentage_error: 11.4159\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8144 - mean_absolute_percentage_error: 8.8144 - val_loss: 11.5091 - val_mean_absolute_percentage_error: 11.5091\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9448 - mean_absolute_percentage_error: 8.9448 - val_loss: 10.6210 - val_mean_absolute_percentage_error: 10.6210\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8522 - mean_absolute_percentage_error: 8.8522 - val_loss: 10.6830 - val_mean_absolute_percentage_error: 10.6830\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1547 - mean_absolute_percentage_error: 9.1547 - val_loss: 11.1530 - val_mean_absolute_percentage_error: 11.1530\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9048 - mean_absolute_percentage_error: 8.9048 - val_loss: 11.1596 - val_mean_absolute_percentage_error: 11.1596\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7786 - mean_absolute_percentage_error: 8.7786 - val_loss: 11.0223 - val_mean_absolute_percentage_error: 11.0223\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6020 - mean_absolute_percentage_error: 8.6020 - val_loss: 10.5519 - val_mean_absolute_percentage_error: 10.5519\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4067 - mean_absolute_percentage_error: 8.4067 - val_loss: 11.4325 - val_mean_absolute_percentage_error: 11.4325\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5600 - mean_absolute_percentage_error: 8.5600 - val_loss: 10.3810 - val_mean_absolute_percentage_error: 10.3810\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4865 - mean_absolute_percentage_error: 8.4865 - val_loss: 10.9155 - val_mean_absolute_percentage_error: 10.9155\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4639 - mean_absolute_percentage_error: 8.4639 - val_loss: 10.9393 - val_mean_absolute_percentage_error: 10.9393\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4357 - mean_absolute_percentage_error: 8.4357 - val_loss: 10.2917 - val_mean_absolute_percentage_error: 10.2917\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3936 - mean_absolute_percentage_error: 8.3936 - val_loss: 11.0359 - val_mean_absolute_percentage_error: 11.0359\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3096 - mean_absolute_percentage_error: 8.3096 - val_loss: 10.8946 - val_mean_absolute_percentage_error: 10.8946\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3320 - mean_absolute_percentage_error: 8.3320 - val_loss: 10.8453 - val_mean_absolute_percentage_error: 10.8453\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2008 - mean_absolute_percentage_error: 8.2008 - val_loss: 10.2285 - val_mean_absolute_percentage_error: 10.2285\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1343 - mean_absolute_percentage_error: 8.1343 - val_loss: 10.7805 - val_mean_absolute_percentage_error: 10.7805\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0952 - mean_absolute_percentage_error: 8.0952 - val_loss: 10.8473 - val_mean_absolute_percentage_error: 10.8473\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1699 - mean_absolute_percentage_error: 8.1699 - val_loss: 10.3651 - val_mean_absolute_percentage_error: 10.3651\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0901 - mean_absolute_percentage_error: 8.0901 - val_loss: 11.0668 - val_mean_absolute_percentage_error: 11.0668\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0393 - mean_absolute_percentage_error: 8.0393 - val_loss: 10.2202 - val_mean_absolute_percentage_error: 10.2202\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0671 - mean_absolute_percentage_error: 8.0671 - val_loss: 10.8634 - val_mean_absolute_percentage_error: 10.8634\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9707 - mean_absolute_percentage_error: 7.9707 - val_loss: 10.2579 - val_mean_absolute_percentage_error: 10.2579\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0481 - mean_absolute_percentage_error: 8.0481 - val_loss: 10.6366 - val_mean_absolute_percentage_error: 10.6366\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8719 - mean_absolute_percentage_error: 7.8719 - val_loss: 10.1630 - val_mean_absolute_percentage_error: 10.1630\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8339 - mean_absolute_percentage_error: 7.8339 - val_loss: 10.1067 - val_mean_absolute_percentage_error: 10.1067\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9029 - mean_absolute_percentage_error: 7.9029 - val_loss: 10.0926 - val_mean_absolute_percentage_error: 10.0926\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9341 - mean_absolute_percentage_error: 7.9341 - val_loss: 10.3335 - val_mean_absolute_percentage_error: 10.3335\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7413 - mean_absolute_percentage_error: 7.7413 - val_loss: 11.0678 - val_mean_absolute_percentage_error: 11.0678\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2364 - mean_absolute_percentage_error: 8.2364 - val_loss: 10.8333 - val_mean_absolute_percentage_error: 10.8333\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9438 - mean_absolute_percentage_error: 7.9438 - val_loss: 10.3961 - val_mean_absolute_percentage_error: 10.3961\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8450 - mean_absolute_percentage_error: 7.8450 - val_loss: 9.9095 - val_mean_absolute_percentage_error: 9.9095\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7300 - mean_absolute_percentage_error: 7.7300 - val_loss: 10.4315 - val_mean_absolute_percentage_error: 10.4315\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6085 - mean_absolute_percentage_error: 7.6085 - val_loss: 10.6133 - val_mean_absolute_percentage_error: 10.6133\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9076 - mean_absolute_percentage_error: 7.9076 - val_loss: 10.6930 - val_mean_absolute_percentage_error: 10.6930\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8243 - mean_absolute_percentage_error: 7.8243 - val_loss: 10.8698 - val_mean_absolute_percentage_error: 10.8698\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3231 - mean_absolute_percentage_error: 8.3231 - val_loss: 10.1180 - val_mean_absolute_percentage_error: 10.1180\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8789 - mean_absolute_percentage_error: 7.8789 - val_loss: 10.5966 - val_mean_absolute_percentage_error: 10.5966\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7689 - mean_absolute_percentage_error: 7.7689 - val_loss: 9.5992 - val_mean_absolute_percentage_error: 9.5992\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8839 - mean_absolute_percentage_error: 7.8839 - val_loss: 10.7797 - val_mean_absolute_percentage_error: 10.7797\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8087 - mean_absolute_percentage_error: 7.8087 - val_loss: 10.4576 - val_mean_absolute_percentage_error: 10.4576\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6783 - mean_absolute_percentage_error: 7.6783 - val_loss: 10.4288 - val_mean_absolute_percentage_error: 10.4288\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3574 - mean_absolute_percentage_error: 7.3574 - val_loss: 9.9635 - val_mean_absolute_percentage_error: 9.9635\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7209 - mean_absolute_percentage_error: 7.7209 - val_loss: 10.3479 - val_mean_absolute_percentage_error: 10.3479\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8406 - mean_absolute_percentage_error: 7.8406 - val_loss: 10.7863 - val_mean_absolute_percentage_error: 10.7863\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6097 - mean_absolute_percentage_error: 7.6097 - val_loss: 9.8737 - val_mean_absolute_percentage_error: 9.8737\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5390 - mean_absolute_percentage_error: 7.5390 - val_loss: 9.9488 - val_mean_absolute_percentage_error: 9.9488\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3826 - mean_absolute_percentage_error: 7.3826 - val_loss: 10.1070 - val_mean_absolute_percentage_error: 10.1070\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2839 - mean_absolute_percentage_error: 7.2839 - val_loss: 9.8678 - val_mean_absolute_percentage_error: 9.8678\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2202 - mean_absolute_percentage_error: 7.2202 - val_loss: 9.9123 - val_mean_absolute_percentage_error: 9.9123\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3378 - mean_absolute_percentage_error: 7.3378 - val_loss: 10.6730 - val_mean_absolute_percentage_error: 10.6730\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8702 - mean_absolute_percentage_error: 7.8702 - val_loss: 10.4013 - val_mean_absolute_percentage_error: 10.4013\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3475 - mean_absolute_percentage_error: 7.3475 - val_loss: 9.8549 - val_mean_absolute_percentage_error: 9.8549\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6306 - mean_absolute_percentage_error: 7.6306 - val_loss: 9.7073 - val_mean_absolute_percentage_error: 9.7073\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4593 - mean_absolute_percentage_error: 7.4593 - val_loss: 9.8766 - val_mean_absolute_percentage_error: 9.8766\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1720 - mean_absolute_percentage_error: 7.1720 - val_loss: 10.0268 - val_mean_absolute_percentage_error: 10.0268\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1744 - mean_absolute_percentage_error: 7.1744 - val_loss: 9.9993 - val_mean_absolute_percentage_error: 9.9993\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9814 - mean_absolute_percentage_error: 6.9814 - val_loss: 10.2465 - val_mean_absolute_percentage_error: 10.2465\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9984 - mean_absolute_percentage_error: 6.9984 - val_loss: 9.8720 - val_mean_absolute_percentage_error: 9.8720\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1710 - mean_absolute_percentage_error: 7.1710 - val_loss: 9.8995 - val_mean_absolute_percentage_error: 9.8995\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0103 - mean_absolute_percentage_error: 7.0103 - val_loss: 9.9220 - val_mean_absolute_percentage_error: 9.9220\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8154 - mean_absolute_percentage_error: 6.8154 - val_loss: 9.7633 - val_mean_absolute_percentage_error: 9.7633\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8674 - mean_absolute_percentage_error: 6.8674 - val_loss: 9.9887 - val_mean_absolute_percentage_error: 9.9887\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8761 - mean_absolute_percentage_error: 6.8761 - val_loss: 10.0555 - val_mean_absolute_percentage_error: 10.0555\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7914 - mean_absolute_percentage_error: 6.7914 - val_loss: 10.0016 - val_mean_absolute_percentage_error: 10.0016\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9108 - mean_absolute_percentage_error: 6.9108 - val_loss: 10.1842 - val_mean_absolute_percentage_error: 10.1842\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8327 - mean_absolute_percentage_error: 6.8327 - val_loss: 10.0424 - val_mean_absolute_percentage_error: 10.0424\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8135 - mean_absolute_percentage_error: 6.8135 - val_loss: 9.9045 - val_mean_absolute_percentage_error: 9.9045\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8141 - mean_absolute_percentage_error: 6.8141 - val_loss: 10.2339 - val_mean_absolute_percentage_error: 10.2339\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9521 - mean_absolute_percentage_error: 6.9521 - val_loss: 9.9046 - val_mean_absolute_percentage_error: 9.9046\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9628 - mean_absolute_percentage_error: 6.9628 - val_loss: 9.8972 - val_mean_absolute_percentage_error: 9.8972\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5736 - mean_absolute_percentage_error: 6.5736 - val_loss: 9.8258 - val_mean_absolute_percentage_error: 9.8258\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5701 - mean_absolute_percentage_error: 6.5701 - val_loss: 9.7244 - val_mean_absolute_percentage_error: 9.7244\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8766 - mean_absolute_percentage_error: 6.8766 - val_loss: 9.9515 - val_mean_absolute_percentage_error: 9.9515\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7116 - mean_absolute_percentage_error: 6.7116 - val_loss: 9.9666 - val_mean_absolute_percentage_error: 9.9666\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7036 - mean_absolute_percentage_error: 6.7036 - val_loss: 9.9183 - val_mean_absolute_percentage_error: 9.9183\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7263 - mean_absolute_percentage_error: 6.7263 - val_loss: 10.0186 - val_mean_absolute_percentage_error: 10.0186\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0271 - mean_absolute_percentage_error: 7.0271 - val_loss: 10.1278 - val_mean_absolute_percentage_error: 10.1278\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7649 - mean_absolute_percentage_error: 6.7649 - val_loss: 10.1915 - val_mean_absolute_percentage_error: 10.1915\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8225 - mean_absolute_percentage_error: 6.8225 - val_loss: 10.2227 - val_mean_absolute_percentage_error: 10.2227\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9003 - mean_absolute_percentage_error: 6.9003 - val_loss: 10.0970 - val_mean_absolute_percentage_error: 10.0970\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5039 - mean_absolute_percentage_error: 6.5039 - val_loss: 9.9381 - val_mean_absolute_percentage_error: 9.9381\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5579 - mean_absolute_percentage_error: 6.5579 - val_loss: 9.7272 - val_mean_absolute_percentage_error: 9.7272\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5052 - mean_absolute_percentage_error: 6.5052 - val_loss: 9.8671 - val_mean_absolute_percentage_error: 9.8671\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7489 - mean_absolute_percentage_error: 6.7489 - val_loss: 10.1076 - val_mean_absolute_percentage_error: 10.1076\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8074 - mean_absolute_percentage_error: 6.8074 - val_loss: 9.9991 - val_mean_absolute_percentage_error: 9.9991\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5887 - mean_absolute_percentage_error: 6.5887 - val_loss: 9.7221 - val_mean_absolute_percentage_error: 9.7221\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5615 - mean_absolute_percentage_error: 6.5615 - val_loss: 10.1485 - val_mean_absolute_percentage_error: 10.1485\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0219 - mean_absolute_percentage_error: 7.0219 - val_loss: 10.6153 - val_mean_absolute_percentage_error: 10.6153\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6319 - mean_absolute_percentage_error: 6.6319 - val_loss: 9.7904 - val_mean_absolute_percentage_error: 9.7904\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4422 - mean_absolute_percentage_error: 6.4422 - val_loss: 9.9476 - val_mean_absolute_percentage_error: 9.9476\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3724 - mean_absolute_percentage_error: 6.3724 - val_loss: 9.8723 - val_mean_absolute_percentage_error: 9.8723\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2543 - mean_absolute_percentage_error: 6.2543 - val_loss: 9.8479 - val_mean_absolute_percentage_error: 9.8479\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2960 - mean_absolute_percentage_error: 6.2960 - val_loss: 10.1230 - val_mean_absolute_percentage_error: 10.1230\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9026 - mean_absolute_percentage_error: 6.9026 - val_loss: 10.2211 - val_mean_absolute_percentage_error: 10.2211\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6801 - mean_absolute_percentage_error: 6.6801 - val_loss: 9.9594 - val_mean_absolute_percentage_error: 9.9594\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5511 - mean_absolute_percentage_error: 6.5511 - val_loss: 9.9725 - val_mean_absolute_percentage_error: 9.9725\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1429 - mean_absolute_percentage_error: 6.1429 - val_loss: 9.8696 - val_mean_absolute_percentage_error: 9.8696\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2202 - mean_absolute_percentage_error: 6.2202 - val_loss: 9.7207 - val_mean_absolute_percentage_error: 9.7207\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4534 - mean_absolute_percentage_error: 6.4534 - val_loss: 9.8519 - val_mean_absolute_percentage_error: 9.8519\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1579 - mean_absolute_percentage_error: 6.1579 - val_loss: 9.8341 - val_mean_absolute_percentage_error: 9.8341\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3526 - mean_absolute_percentage_error: 6.3526 - val_loss: 9.8113 - val_mean_absolute_percentage_error: 9.8113\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2772 - mean_absolute_percentage_error: 6.2772 - val_loss: 9.9194 - val_mean_absolute_percentage_error: 9.9194\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2303 - mean_absolute_percentage_error: 6.2303 - val_loss: 9.7127 - val_mean_absolute_percentage_error: 9.7127\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4312 - mean_absolute_percentage_error: 6.4312 - val_loss: 9.6791 - val_mean_absolute_percentage_error: 9.6791\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1997 - mean_absolute_percentage_error: 6.1997 - val_loss: 9.8865 - val_mean_absolute_percentage_error: 9.8865\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1503 - mean_absolute_percentage_error: 6.1503 - val_loss: 9.8408 - val_mean_absolute_percentage_error: 9.8408\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2417 - mean_absolute_percentage_error: 6.2417 - val_loss: 9.7925 - val_mean_absolute_percentage_error: 9.7925\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0781 - mean_absolute_percentage_error: 6.0781 - val_loss: 9.7517 - val_mean_absolute_percentage_error: 9.7517\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1237 - mean_absolute_percentage_error: 6.1237 - val_loss: 9.6972 - val_mean_absolute_percentage_error: 9.6972\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9526 - mean_absolute_percentage_error: 5.9526 - val_loss: 9.7292 - val_mean_absolute_percentage_error: 9.7292\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1531 - mean_absolute_percentage_error: 6.1531 - val_loss: 10.0103 - val_mean_absolute_percentage_error: 10.0103\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7887 - mean_absolute_percentage_error: 6.7887 - val_loss: 9.7473 - val_mean_absolute_percentage_error: 9.7473\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6764 - mean_absolute_percentage_error: 6.6764 - val_loss: 9.8529 - val_mean_absolute_percentage_error: 9.8529\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7649 - mean_absolute_percentage_error: 6.7649 - val_loss: 9.5876 - val_mean_absolute_percentage_error: 9.5876\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3229 - mean_absolute_percentage_error: 6.3229 - val_loss: 9.6624 - val_mean_absolute_percentage_error: 9.6624\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2760 - mean_absolute_percentage_error: 6.2760 - val_loss: 9.9020 - val_mean_absolute_percentage_error: 9.9020\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0600 - mean_absolute_percentage_error: 6.0600 - val_loss: 10.0249 - val_mean_absolute_percentage_error: 10.0249\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1237 - mean_absolute_percentage_error: 6.1237 - val_loss: 9.7428 - val_mean_absolute_percentage_error: 9.7428\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8870 - mean_absolute_percentage_error: 5.8870 - val_loss: 9.6448 - val_mean_absolute_percentage_error: 9.6448\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0839 - mean_absolute_percentage_error: 6.0839 - val_loss: 9.9831 - val_mean_absolute_percentage_error: 9.9831\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0856 - mean_absolute_percentage_error: 6.0856 - val_loss: 9.6884 - val_mean_absolute_percentage_error: 9.6884\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0969 - mean_absolute_percentage_error: 6.0969 - val_loss: 9.6285 - val_mean_absolute_percentage_error: 9.6285\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0654 - mean_absolute_percentage_error: 6.0654 - val_loss: 9.6546 - val_mean_absolute_percentage_error: 9.6546\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2057 - mean_absolute_percentage_error: 6.2057 - val_loss: 9.9013 - val_mean_absolute_percentage_error: 9.9013\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0853 - mean_absolute_percentage_error: 6.0853 - val_loss: 9.4622 - val_mean_absolute_percentage_error: 9.4622\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8508 - mean_absolute_percentage_error: 5.8508 - val_loss: 9.5026 - val_mean_absolute_percentage_error: 9.5026\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8302 - mean_absolute_percentage_error: 5.8302 - val_loss: 9.6302 - val_mean_absolute_percentage_error: 9.6302\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0539 - mean_absolute_percentage_error: 6.0539 - val_loss: 9.3481 - val_mean_absolute_percentage_error: 9.3481\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1254 - mean_absolute_percentage_error: 6.1254 - val_loss: 9.5469 - val_mean_absolute_percentage_error: 9.5469\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0728 - mean_absolute_percentage_error: 6.0728 - val_loss: 9.3461 - val_mean_absolute_percentage_error: 9.3461\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7718 - mean_absolute_percentage_error: 5.7718 - val_loss: 9.6737 - val_mean_absolute_percentage_error: 9.6737\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9007 - mean_absolute_percentage_error: 5.9007 - val_loss: 9.4632 - val_mean_absolute_percentage_error: 9.4632\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0880 - mean_absolute_percentage_error: 6.0880 - val_loss: 9.3076 - val_mean_absolute_percentage_error: 9.3076\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2247 - mean_absolute_percentage_error: 6.2247 - val_loss: 9.3487 - val_mean_absolute_percentage_error: 9.3487\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9286 - mean_absolute_percentage_error: 5.9286 - val_loss: 9.4245 - val_mean_absolute_percentage_error: 9.4245\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0564 - mean_absolute_percentage_error: 6.0564 - val_loss: 9.2164 - val_mean_absolute_percentage_error: 9.2164\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6941 - mean_absolute_percentage_error: 5.6941 - val_loss: 9.5592 - val_mean_absolute_percentage_error: 9.5592\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9220 - mean_absolute_percentage_error: 5.9220 - val_loss: 9.4910 - val_mean_absolute_percentage_error: 9.4910\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2405 - mean_absolute_percentage_error: 6.2405 - val_loss: 9.4140 - val_mean_absolute_percentage_error: 9.4140\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9495 - mean_absolute_percentage_error: 5.9495 - val_loss: 9.3193 - val_mean_absolute_percentage_error: 9.3193\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9191 - mean_absolute_percentage_error: 5.9191 - val_loss: 9.4974 - val_mean_absolute_percentage_error: 9.4974\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8490 - mean_absolute_percentage_error: 5.8490 - val_loss: 9.3773 - val_mean_absolute_percentage_error: 9.3773\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6492 - mean_absolute_percentage_error: 5.6492 - val_loss: 9.3974 - val_mean_absolute_percentage_error: 9.3974\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8675 - mean_absolute_percentage_error: 5.8675 - val_loss: 9.5192 - val_mean_absolute_percentage_error: 9.5192\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5495 - mean_absolute_percentage_error: 6.5495 - val_loss: 9.1926 - val_mean_absolute_percentage_error: 9.1926\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1099 - mean_absolute_percentage_error: 6.1099 - val_loss: 9.3934 - val_mean_absolute_percentage_error: 9.3934\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8016 - mean_absolute_percentage_error: 5.8016 - val_loss: 9.3972 - val_mean_absolute_percentage_error: 9.3972\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8512 - mean_absolute_percentage_error: 5.8512 - val_loss: 9.2595 - val_mean_absolute_percentage_error: 9.2595\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7434 - mean_absolute_percentage_error: 5.7434 - val_loss: 9.2408 - val_mean_absolute_percentage_error: 9.2408\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6552 - mean_absolute_percentage_error: 5.6552 - val_loss: 9.3121 - val_mean_absolute_percentage_error: 9.3121\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0785 - mean_absolute_percentage_error: 6.0785 - val_loss: 9.4545 - val_mean_absolute_percentage_error: 9.4545\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9593 - mean_absolute_percentage_error: 5.9593 - val_loss: 9.6219 - val_mean_absolute_percentage_error: 9.6219\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7497 - mean_absolute_percentage_error: 5.7497 - val_loss: 9.3585 - val_mean_absolute_percentage_error: 9.3585\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7291 - mean_absolute_percentage_error: 5.7291 - val_loss: 9.3087 - val_mean_absolute_percentage_error: 9.3087\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6922 - mean_absolute_percentage_error: 5.6922 - val_loss: 9.3998 - val_mean_absolute_percentage_error: 9.3998\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8811 - mean_absolute_percentage_error: 5.8811 - val_loss: 9.3140 - val_mean_absolute_percentage_error: 9.3140\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8225 - mean_absolute_percentage_error: 5.8225 - val_loss: 9.5101 - val_mean_absolute_percentage_error: 9.5101\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8540 - mean_absolute_percentage_error: 5.8540 - val_loss: 9.4570 - val_mean_absolute_percentage_error: 9.4570\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6950 - mean_absolute_percentage_error: 5.6950 - val_loss: 9.1696 - val_mean_absolute_percentage_error: 9.1696\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6309 - mean_absolute_percentage_error: 5.6309 - val_loss: 9.3924 - val_mean_absolute_percentage_error: 9.3924\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0079 - mean_absolute_percentage_error: 6.0079 - val_loss: 9.5607 - val_mean_absolute_percentage_error: 9.5607\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1464 - mean_absolute_percentage_error: 6.1464 - val_loss: 9.2051 - val_mean_absolute_percentage_error: 9.2051\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8883 - mean_absolute_percentage_error: 5.8883 - val_loss: 9.2804 - val_mean_absolute_percentage_error: 9.2804\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7345 - mean_absolute_percentage_error: 5.7345 - val_loss: 9.2020 - val_mean_absolute_percentage_error: 9.2020\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5808 - mean_absolute_percentage_error: 5.5808 - val_loss: 9.3607 - val_mean_absolute_percentage_error: 9.3607\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5535 - mean_absolute_percentage_error: 5.5535 - val_loss: 9.2084 - val_mean_absolute_percentage_error: 9.2084\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5046 - mean_absolute_percentage_error: 5.5046 - val_loss: 9.0880 - val_mean_absolute_percentage_error: 9.0880\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4435 - mean_absolute_percentage_error: 5.4435 - val_loss: 9.3167 - val_mean_absolute_percentage_error: 9.3167\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3989 - mean_absolute_percentage_error: 5.3989 - val_loss: 9.0619 - val_mean_absolute_percentage_error: 9.0619\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4347 - mean_absolute_percentage_error: 5.4347 - val_loss: 9.0269 - val_mean_absolute_percentage_error: 9.0269\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3852 - mean_absolute_percentage_error: 5.3852 - val_loss: 8.9894 - val_mean_absolute_percentage_error: 8.9894\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4812 - mean_absolute_percentage_error: 5.4812 - val_loss: 9.2439 - val_mean_absolute_percentage_error: 9.2439\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7168 - mean_absolute_percentage_error: 5.7168 - val_loss: 9.0187 - val_mean_absolute_percentage_error: 9.0187\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5943 - mean_absolute_percentage_error: 5.5943 - val_loss: 9.0948 - val_mean_absolute_percentage_error: 9.0948\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5302 - mean_absolute_percentage_error: 5.5302 - val_loss: 8.8965 - val_mean_absolute_percentage_error: 8.8965\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6654 - mean_absolute_percentage_error: 5.6654 - val_loss: 9.2869 - val_mean_absolute_percentage_error: 9.2869\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6979 - mean_absolute_percentage_error: 5.6979 - val_loss: 9.0061 - val_mean_absolute_percentage_error: 9.0061\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5898 - mean_absolute_percentage_error: 5.5898 - val_loss: 8.9707 - val_mean_absolute_percentage_error: 8.9707\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4346 - mean_absolute_percentage_error: 5.4346 - val_loss: 8.9707 - val_mean_absolute_percentage_error: 8.9707\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4049 - mean_absolute_percentage_error: 5.4049 - val_loss: 8.9840 - val_mean_absolute_percentage_error: 8.9840\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2750 - mean_absolute_percentage_error: 5.2750 - val_loss: 8.8563 - val_mean_absolute_percentage_error: 8.8563\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5116 - mean_absolute_percentage_error: 5.5116 - val_loss: 8.9468 - val_mean_absolute_percentage_error: 8.9468\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6101 - mean_absolute_percentage_error: 5.6101 - val_loss: 9.0691 - val_mean_absolute_percentage_error: 9.0691\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5714 - mean_absolute_percentage_error: 5.5714 - val_loss: 8.8831 - val_mean_absolute_percentage_error: 8.8831\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3697 - mean_absolute_percentage_error: 5.3697 - val_loss: 8.7672 - val_mean_absolute_percentage_error: 8.7672\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3999 - mean_absolute_percentage_error: 5.3999 - val_loss: 9.1090 - val_mean_absolute_percentage_error: 9.1090\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5068 - mean_absolute_percentage_error: 5.5068 - val_loss: 9.1120 - val_mean_absolute_percentage_error: 9.1120\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3911 - mean_absolute_percentage_error: 5.3911 - val_loss: 9.0236 - val_mean_absolute_percentage_error: 9.0236\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5162 - mean_absolute_percentage_error: 5.5162 - val_loss: 8.8618 - val_mean_absolute_percentage_error: 8.8618\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2294 - mean_absolute_percentage_error: 5.2294 - val_loss: 9.0760 - val_mean_absolute_percentage_error: 9.0760\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2685 - mean_absolute_percentage_error: 5.2685 - val_loss: 9.0018 - val_mean_absolute_percentage_error: 9.0018\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2700 - mean_absolute_percentage_error: 5.2700 - val_loss: 8.8004 - val_mean_absolute_percentage_error: 8.8004\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3514 - mean_absolute_percentage_error: 5.3514 - val_loss: 9.2462 - val_mean_absolute_percentage_error: 9.2462\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4379 - mean_absolute_percentage_error: 5.4379 - val_loss: 8.8597 - val_mean_absolute_percentage_error: 8.8597\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4324 - mean_absolute_percentage_error: 5.4324 - val_loss: 8.7549 - val_mean_absolute_percentage_error: 8.7549\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3136 - mean_absolute_percentage_error: 5.3136 - val_loss: 8.5225 - val_mean_absolute_percentage_error: 8.5225\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4162 - mean_absolute_percentage_error: 5.4162 - val_loss: 8.6358 - val_mean_absolute_percentage_error: 8.6358\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.7344 - mean_absolute_percentage_error: 5.7344 - val_loss: 9.1158 - val_mean_absolute_percentage_error: 9.1158\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6290 - mean_absolute_percentage_error: 5.6290 - val_loss: 8.9302 - val_mean_absolute_percentage_error: 8.9302\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2606 - mean_absolute_percentage_error: 5.2606 - val_loss: 8.6455 - val_mean_absolute_percentage_error: 8.6455\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1493 - mean_absolute_percentage_error: 5.1493 - val_loss: 8.7852 - val_mean_absolute_percentage_error: 8.7852\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2962 - mean_absolute_percentage_error: 5.2962 - val_loss: 8.8212 - val_mean_absolute_percentage_error: 8.8212\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1673 - mean_absolute_percentage_error: 5.1673 - val_loss: 8.6435 - val_mean_absolute_percentage_error: 8.6435\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3095 - mean_absolute_percentage_error: 5.3095 - val_loss: 8.8960 - val_mean_absolute_percentage_error: 8.8960\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3110 - mean_absolute_percentage_error: 5.3110 - val_loss: 8.7356 - val_mean_absolute_percentage_error: 8.7356\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2656 - mean_absolute_percentage_error: 5.2656 - val_loss: 8.9065 - val_mean_absolute_percentage_error: 8.9065\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2471 - mean_absolute_percentage_error: 5.2471 - val_loss: 8.9610 - val_mean_absolute_percentage_error: 8.9610\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3968 - mean_absolute_percentage_error: 5.3968 - val_loss: 8.9566 - val_mean_absolute_percentage_error: 8.9566\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5032 - mean_absolute_percentage_error: 5.5032 - val_loss: 8.6660 - val_mean_absolute_percentage_error: 8.6660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8830de8350>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ede529-969a-48c0-b5aa-868fa02f4c31"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bbdc9d-d739-43d0-fd44-1636c7dc4853"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 16s 5ms/step - loss: 437.4924 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0143 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0071 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0069 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0106 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0092 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0120 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0113 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0120 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0144 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0122 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0119 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0129 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4925 - accuracy: 0.0135 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0146 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0124 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0145 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0159 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0156 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4925 - accuracy: 0.0140 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0139 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0115 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0121 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4926 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0116 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0130 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4916 - accuracy: 0.0088 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0115 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0106 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0084 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4918 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0101 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0086 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4918 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88303a8990>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e63723-1c25-4c04-e144-90aa06f932da"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39dac01-168b-44ff-be2b-eba8e5b92b24"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0428 - accuracy: 0.1449 - val_loss: 0.0344 - val_accuracy: 0.2279\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0332 - accuracy: 0.2643 - val_loss: 0.0322 - val_accuracy: 0.2985\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0314 - accuracy: 0.3110 - val_loss: 0.0307 - val_accuracy: 0.3277\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0301 - accuracy: 0.3431 - val_loss: 0.0297 - val_accuracy: 0.3551\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0291 - accuracy: 0.3696 - val_loss: 0.0295 - val_accuracy: 0.3575\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0284 - accuracy: 0.3867 - val_loss: 0.0283 - val_accuracy: 0.3924\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0277 - accuracy: 0.4083 - val_loss: 0.0282 - val_accuracy: 0.3983\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0271 - accuracy: 0.4192 - val_loss: 0.0273 - val_accuracy: 0.4108\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0267 - accuracy: 0.4318 - val_loss: 0.0272 - val_accuracy: 0.4236\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0261 - accuracy: 0.4440 - val_loss: 0.0269 - val_accuracy: 0.4222\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0257 - accuracy: 0.4538 - val_loss: 0.0267 - val_accuracy: 0.4298\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0254 - accuracy: 0.4617 - val_loss: 0.0268 - val_accuracy: 0.4306\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0250 - accuracy: 0.4708 - val_loss: 0.0262 - val_accuracy: 0.4397\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0247 - accuracy: 0.4802 - val_loss: 0.0261 - val_accuracy: 0.4458\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0244 - accuracy: 0.4840 - val_loss: 0.0258 - val_accuracy: 0.4571\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0242 - accuracy: 0.4898 - val_loss: 0.0262 - val_accuracy: 0.4489\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0240 - accuracy: 0.4957 - val_loss: 0.0258 - val_accuracy: 0.4574\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0238 - accuracy: 0.5015 - val_loss: 0.0257 - val_accuracy: 0.4625\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0236 - accuracy: 0.5072 - val_loss: 0.0256 - val_accuracy: 0.4619\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0234 - accuracy: 0.5102 - val_loss: 0.0256 - val_accuracy: 0.4603\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0233 - accuracy: 0.5129 - val_loss: 0.0257 - val_accuracy: 0.4598\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0231 - accuracy: 0.5180 - val_loss: 0.0257 - val_accuracy: 0.4620\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0230 - accuracy: 0.5175 - val_loss: 0.0254 - val_accuracy: 0.4635\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0228 - accuracy: 0.5240 - val_loss: 0.0255 - val_accuracy: 0.4671\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0228 - accuracy: 0.5279 - val_loss: 0.0252 - val_accuracy: 0.4728\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0226 - accuracy: 0.5303 - val_loss: 0.0255 - val_accuracy: 0.4699\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0225 - accuracy: 0.5335 - val_loss: 0.0252 - val_accuracy: 0.4759\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0224 - accuracy: 0.5358 - val_loss: 0.0252 - val_accuracy: 0.4731\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0223 - accuracy: 0.5360 - val_loss: 0.0255 - val_accuracy: 0.4737\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0221 - accuracy: 0.5418 - val_loss: 0.0251 - val_accuracy: 0.4744\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0220 - accuracy: 0.5407 - val_loss: 0.0255 - val_accuracy: 0.4676\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0219 - accuracy: 0.5462 - val_loss: 0.0250 - val_accuracy: 0.4807\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0218 - accuracy: 0.5493 - val_loss: 0.0254 - val_accuracy: 0.4746\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0217 - accuracy: 0.5515 - val_loss: 0.0252 - val_accuracy: 0.4744\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0216 - accuracy: 0.5525 - val_loss: 0.0253 - val_accuracy: 0.4753\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0215 - accuracy: 0.5554 - val_loss: 0.0267 - val_accuracy: 0.4479\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0214 - accuracy: 0.5571 - val_loss: 0.0252 - val_accuracy: 0.4796\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0214 - accuracy: 0.5576 - val_loss: 0.0257 - val_accuracy: 0.4734\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0212 - accuracy: 0.5619 - val_loss: 0.0263 - val_accuracy: 0.4574\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.5617 - val_loss: 0.0255 - val_accuracy: 0.4780\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.5624 - val_loss: 0.0255 - val_accuracy: 0.4783\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0210 - accuracy: 0.5649 - val_loss: 0.0254 - val_accuracy: 0.4793\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0210 - accuracy: 0.5675 - val_loss: 0.0255 - val_accuracy: 0.4797\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0209 - accuracy: 0.5684 - val_loss: 0.0252 - val_accuracy: 0.4811\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0208 - accuracy: 0.5707 - val_loss: 0.0258 - val_accuracy: 0.4743\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0208 - accuracy: 0.5723 - val_loss: 0.0261 - val_accuracy: 0.4667\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0207 - accuracy: 0.5755 - val_loss: 0.0256 - val_accuracy: 0.4702\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0207 - accuracy: 0.5750 - val_loss: 0.0254 - val_accuracy: 0.4821\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0206 - accuracy: 0.5772 - val_loss: 0.0255 - val_accuracy: 0.4763\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0205 - accuracy: 0.5797 - val_loss: 0.0257 - val_accuracy: 0.4741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f881cad1890>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe."
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 7\n",
        "\n",
        "The above dataset has a 100 classes, while binary cross entropy is only used for binary classification, where there are only two classes. "
      ],
      "metadata": {
        "id": "Wq-LRQ2MlVfA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad97e2b-1a04-4673-f5e3-014ec6eb3a85"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6007 - accuracy: 0.1977 - val_loss: 2.3245 - val_accuracy: 0.2770\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2428 - accuracy: 0.3032 - val_loss: 2.2092 - val_accuracy: 0.3196\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0970 - accuracy: 0.3473 - val_loss: 2.0552 - val_accuracy: 0.3563\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9976 - accuracy: 0.3772 - val_loss: 2.0035 - val_accuracy: 0.3759\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9239 - accuracy: 0.3995 - val_loss: 1.9340 - val_accuracy: 0.3966\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8657 - accuracy: 0.4145 - val_loss: 1.9221 - val_accuracy: 0.3985\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8147 - accuracy: 0.4321 - val_loss: 1.8841 - val_accuracy: 0.4086\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7782 - accuracy: 0.4441 - val_loss: 1.8387 - val_accuracy: 0.4277\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7449 - accuracy: 0.4548 - val_loss: 1.8733 - val_accuracy: 0.4166\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7137 - accuracy: 0.4627 - val_loss: 1.8565 - val_accuracy: 0.4250\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6846 - accuracy: 0.4704 - val_loss: 1.7849 - val_accuracy: 0.4410\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6607 - accuracy: 0.4805 - val_loss: 1.8034 - val_accuracy: 0.4387\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6343 - accuracy: 0.4842 - val_loss: 1.7680 - val_accuracy: 0.4535\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6127 - accuracy: 0.4925 - val_loss: 1.7572 - val_accuracy: 0.4573\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5991 - accuracy: 0.4963 - val_loss: 1.7853 - val_accuracy: 0.4419\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5743 - accuracy: 0.5028 - val_loss: 1.7398 - val_accuracy: 0.4585\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5560 - accuracy: 0.5081 - val_loss: 1.7366 - val_accuracy: 0.4593\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5424 - accuracy: 0.5109 - val_loss: 1.7722 - val_accuracy: 0.4564\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5243 - accuracy: 0.5176 - val_loss: 1.7393 - val_accuracy: 0.4529\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5132 - accuracy: 0.5211 - val_loss: 1.7414 - val_accuracy: 0.4657\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4973 - accuracy: 0.5250 - val_loss: 1.7893 - val_accuracy: 0.4518\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4893 - accuracy: 0.5269 - val_loss: 1.7642 - val_accuracy: 0.4583\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4736 - accuracy: 0.5339 - val_loss: 1.7593 - val_accuracy: 0.4613\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4611 - accuracy: 0.5363 - val_loss: 1.7523 - val_accuracy: 0.4672\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4510 - accuracy: 0.5386 - val_loss: 1.8225 - val_accuracy: 0.4516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f881c122a10>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    sum = []\n",
        "    for i in range(len(true)):\n",
        "      sum.append(1 * np.log(pred[i][np.argmax(true[i])]))\n",
        "    sum = np.sum(np.array(sum))\n",
        "    loss = (-1/len(true)) * sum\n",
        "\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2bd09c-8532-4546-818a-a53fd2f0cdec"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3445815332233906\n",
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db938f5-ae77-4938-95a9-4be945b2dbb8"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 2.5481 - accuracy: 0.2148 - val_loss: 2.3005 - val_accuracy: 0.2929\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1828 - accuracy: 0.3248 - val_loss: 2.1445 - val_accuracy: 0.3255\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0263 - accuracy: 0.3699 - val_loss: 2.0196 - val_accuracy: 0.3721\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9242 - accuracy: 0.4007 - val_loss: 1.9175 - val_accuracy: 0.3992\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8498 - accuracy: 0.4225 - val_loss: 1.8720 - val_accuracy: 0.4203\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7904 - accuracy: 0.4406 - val_loss: 1.8344 - val_accuracy: 0.4300\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7401 - accuracy: 0.4543 - val_loss: 1.8093 - val_accuracy: 0.4458\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6961 - accuracy: 0.4665 - val_loss: 1.8091 - val_accuracy: 0.4392\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6590 - accuracy: 0.4787 - val_loss: 1.7713 - val_accuracy: 0.4492\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6273 - accuracy: 0.4904 - val_loss: 1.7767 - val_accuracy: 0.4501\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6005 - accuracy: 0.4950 - val_loss: 1.7573 - val_accuracy: 0.4535\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5774 - accuracy: 0.5019 - val_loss: 1.7513 - val_accuracy: 0.4633\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5495 - accuracy: 0.5103 - val_loss: 1.8033 - val_accuracy: 0.4545\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5258 - accuracy: 0.5165 - val_loss: 1.7378 - val_accuracy: 0.4591\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5068 - accuracy: 0.5248 - val_loss: 1.7383 - val_accuracy: 0.4693\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4871 - accuracy: 0.5286 - val_loss: 1.7292 - val_accuracy: 0.4698\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4659 - accuracy: 0.5350 - val_loss: 1.7630 - val_accuracy: 0.4605\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4517 - accuracy: 0.5396 - val_loss: 1.7373 - val_accuracy: 0.4730\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4358 - accuracy: 0.5445 - val_loss: 1.7584 - val_accuracy: 0.4655\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4216 - accuracy: 0.5472 - val_loss: 1.7532 - val_accuracy: 0.4733\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4077 - accuracy: 0.5506 - val_loss: 1.7273 - val_accuracy: 0.4747\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3965 - accuracy: 0.5541 - val_loss: 1.7878 - val_accuracy: 0.4606\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3799 - accuracy: 0.5589 - val_loss: 1.7522 - val_accuracy: 0.4722\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3690 - accuracy: 0.5629 - val_loss: 1.7848 - val_accuracy: 0.4624\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3609 - accuracy: 0.5646 - val_loss: 1.7697 - val_accuracy: 0.4678\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3456 - accuracy: 0.5690 - val_loss: 1.7633 - val_accuracy: 0.4768\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3337 - accuracy: 0.5735 - val_loss: 1.8358 - val_accuracy: 0.4566\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3275 - accuracy: 0.5740 - val_loss: 1.8251 - val_accuracy: 0.4565\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3149 - accuracy: 0.5775 - val_loss: 1.7725 - val_accuracy: 0.4755\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3044 - accuracy: 0.5793 - val_loss: 1.7915 - val_accuracy: 0.4720\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2953 - accuracy: 0.5839 - val_loss: 1.8316 - val_accuracy: 0.4671\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2883 - accuracy: 0.5863 - val_loss: 1.8247 - val_accuracy: 0.4638\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2793 - accuracy: 0.5882 - val_loss: 1.8075 - val_accuracy: 0.4729\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2687 - accuracy: 0.5920 - val_loss: 1.8786 - val_accuracy: 0.4652\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2663 - accuracy: 0.5928 - val_loss: 1.8586 - val_accuracy: 0.4644\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2554 - accuracy: 0.5952 - val_loss: 1.8511 - val_accuracy: 0.4703\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2484 - accuracy: 0.5970 - val_loss: 1.8233 - val_accuracy: 0.4722\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2458 - accuracy: 0.5992 - val_loss: 1.8186 - val_accuracy: 0.4718\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2331 - accuracy: 0.6025 - val_loss: 1.8243 - val_accuracy: 0.4724\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2267 - accuracy: 0.6044 - val_loss: 1.8538 - val_accuracy: 0.4691\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2220 - accuracy: 0.6045 - val_loss: 1.8945 - val_accuracy: 0.4617\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2149 - accuracy: 0.6061 - val_loss: 1.8564 - val_accuracy: 0.4659\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2056 - accuracy: 0.6102 - val_loss: 1.8854 - val_accuracy: 0.4600\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2019 - accuracy: 0.6122 - val_loss: 1.8599 - val_accuracy: 0.4739\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1940 - accuracy: 0.6135 - val_loss: 1.9244 - val_accuracy: 0.4580\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1924 - accuracy: 0.6150 - val_loss: 1.8878 - val_accuracy: 0.4686\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1924 - accuracy: 0.6154 - val_loss: 1.8798 - val_accuracy: 0.4662\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1829 - accuracy: 0.6170 - val_loss: 1.9181 - val_accuracy: 0.4594\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1795 - accuracy: 0.6169 - val_loss: 1.9775 - val_accuracy: 0.4567\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1734 - accuracy: 0.6205 - val_loss: 1.9348 - val_accuracy: 0.4714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f883003c6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 9\n",
        "Multiclass classification problem consists of one sample corresponding to a single label, while multilabel classification task can have one sample correspond to more than one label. The suitable loss function would be Categorical cross entropy. "
      ],
      "metadata": {
        "id": "GRiNFcG5xpz1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 10\n",
        "\n",
        "Both of them compare the output probabilities with the true label, and use log function in their calculation. They are both used in a classification task, but Binary cross entropy is only used in a binary classification. They both use one-hot encoded array for predicted labels. "
      ],
      "metadata": {
        "id": "yvvaeXPB0Trw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer 11\n",
        "\n",
        "They are both used in multiclass classification tasks, and use log function in their calculation. They are both suitable for more than 2 classes, but Sparse cross entropy does not use one-hot encoded inputs. "
      ],
      "metadata": {
        "id": "C4tmcID41iHj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}